{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "from scipy import stats\n",
    "import collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sia.transformers import as_window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ds = load_dataset('csv', data_files='./data/ecg_model/30100.csv')['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import neurokit2 as nk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n = 20 * 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# window = ds[0:0+n]['signal']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# peaks, info = nk.ecg_peaks(window, sampling_rate=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ECG_Rate_Mean, ECG_Rate_STD\n",
    "# meanHR = np.mean(nk.signal_rate(peaks, sampling_rate=1000))\n",
    "# stdHR = np.std(nk.signal_rate(peaks, sampling_rate=1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# peak_indices = peaks[peaks['ECG_R_Peaks'] == 1].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# periods=np.array([(peak_indices[i+1]-peak_indices[i])/1000 for i in range(0,len(peak_indices)-1)])\n",
    "# frequency=1/periods\n",
    "# meanfreq = np.mean(frequency)\n",
    "# stdfreq = np.std(frequency)\n",
    "# frequency, meanfreq, stdfreq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hrv = np.array([(peak_indices[i]-peak_indices[i-1])/1000 for i in range(1,len(peak_indices))])\n",
    "# meanHRV=np.mean(hrv)\n",
    "# stdHRV=np.std(hrv)\n",
    "# rmsHRV=np.sqrt(np.mean(hrv**2))\n",
    "# meanHRV, stdHRV, rmsHRV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "from scipy import stats\n",
    "\n",
    "def TINN(x:np.array):\n",
    "  \"\"\" Compute all the triangular interpolation to calculate the TINN scores. It also computes HRV index from an array x which contains \n",
    "      all the interbeats times for a given ECG signal.\n",
    "\n",
    "      The axis is divided in 2 parts respectively on the right and left of the abscissa of the maximum value of the gaussian distribution\n",
    "      The TINN score calculation is defined in the WESAD Dataset paper, to calculate it we needthe closest triangular interpolation \n",
    "      of the gaussian distribution of the interbeats times. The triangular interpolation is defined by 2 lines that meet at the maximum value\n",
    "      of the gaussian distribution and cross the x-axis in N on the first half of the x-axis and M on the second half of the x-axis. \n",
    "      Thus inside ]N;M[ the interpolation function != 0\n",
    "      Outside of ]N;M[ the interpolation function equals 0.\n",
    "  \"\"\"\n",
    "\n",
    "  kernel = stats.gaussian_kde(x) #Create an approximated kernel for gaussian distribution from the x array (interbeats times)\n",
    "  absi=np.linspace(np.min(x),np.max(x),len(x)) # Compute the x-axis of the interbeats distribution (from minimum interbeat time to maximum interbeat time)\n",
    "  val=kernel.evaluate(absi) # Fit the gaussian distribution to the created x-axis\n",
    "  ecart=absi[1]-absi[0] # Space between 2 values on the axis\n",
    "  maxind=np.argmax(val) # Select the index for which the gaussian distribution (val array) is maximum \n",
    "  max_pos=absi[maxind]  # Interbeat time (abscissa) for which the gaussian distribution is maximum\n",
    "  maxvalue=np.amax(val) # Max of the gaussian distribution\n",
    "  N_abs=absi[0:maxind+1] # First half of the x-axis\n",
    "  M_abs=absi[maxind:] # Second half of the x-axis\n",
    "  HRVindex=len(x)/maxvalue\n",
    "  err_N=[]\n",
    "  err_M=[]\n",
    "\n",
    "  for i in range(0,len(N_abs)-1):\n",
    "    N=N_abs[i]\n",
    "    slope=(maxvalue)/(max_pos-N)\n",
    "    D=val[0:maxind+1]\n",
    "    q=np.clip(slope*ecart*np.arange(-i,-i+maxind+1),0,None) #Triangular interpolation on the First half of the x-axis\n",
    "    diff=D-q \n",
    "    err=np.multiply(diff,diff)\n",
    "    err1=np.delete(err,-1)\n",
    "    err2=np.delete(err, 0)\n",
    "    errint=(err1+err2)/2\n",
    "    errtot=np.linalg.norm(errint) # Error area between the triangular interpolation and the gaussian distribution on the first half of the x-axis\n",
    "    err_N.append((errtot,N,N_abs,q))\n",
    "  \n",
    "  for i in range(1,len(M_abs)):\n",
    "    M=M_abs[i]\n",
    "    slope=(maxvalue)/(max_pos-M)\n",
    "    D=val[maxind:]\n",
    "    q=np.clip(slope*ecart*np.arange(-i,len(D)-i),0,None) #Triangular interpolation on the second half of the x-axis\n",
    "    diff=D-q\n",
    "    err=np.multiply(diff,diff)\n",
    "    err1=np.delete(err,-1)\n",
    "    err2=np.delete(err, 0)\n",
    "    errint=(err1+err2)/2\n",
    "    errtot=np.linalg.norm(errint) # Error area between the triangular interpolation and the gaussian distribution on the second half of the x-axis\n",
    "    err_M.append((errtot,M,M_abs,q))\n",
    "\n",
    "  return (err_N,err_M,absi,val,HRVindex)\n",
    "\n",
    "def best_TINN(x:np.array):\n",
    "  \"\"\"Select the best N and M that give the best triangular interpolation function approximation of the gaussian distrbution and return\n",
    "    N; M; the TINN score = M-N ; and the HRV index\n",
    "  \n",
    "  \"\"\"\n",
    "  err_N,err_M,_,_,HRVindex=TINN(x)\n",
    "  N=np.argmin(np.array(err_N,dtype=object)[:,0])\n",
    "  M=np.argmin(np.array(err_M,dtype=object)[:,0])\n",
    "  absN=err_N[N][1]\n",
    "  absM=err_M[M][1]\n",
    "  return float(absN),float(absM),float(absM-absN),HRVindex\n",
    "\n",
    "# _,_,T,HRVindex=best_TINN(hrv)\n",
    "# T, HRVindex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_compare_NN50(x,i):\n",
    "  \"\"\"Count the number of HRV intervals differing more than 50 ms for a given HRV interval x[i]\n",
    "  \n",
    "  \"\"\"\n",
    "  ref=x[i]\n",
    "  k=0\n",
    "  diff=np.absolute(x-ref)\n",
    "  k+=np.sum(np.where(diff>0.05,1,0))\n",
    "  return k \n",
    "\n",
    "def compare_NN50(x):\n",
    "  \"\"\" Returns the number and percentage of HRV intervals differing more than 50ms for all intervals\n",
    "  \n",
    "  \"\"\"\n",
    "  k=0\n",
    "  for i in range(0,len(x)):\n",
    "    k+=num_compare_NN50(x,i)\n",
    "  if k==0:\n",
    "    k=1\n",
    "  return k,(k/(len(x)*len(x)))\n",
    "\n",
    "# num50,p50=compare_NN50(hrv)\n",
    "# num50, p50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_freq_features_ecg(x):\n",
    "  \"\"\" Returns frequential features of the Heart Rate Variability signal (interbeats times) by computing FFT, to compute the Fouriers \n",
    "  Frequencies the mean of the Heart Rate variability is used as sampling period  \n",
    "  \"\"\"\n",
    "  mean=np.mean(x)\n",
    "  yf=np.array(scipy.fft.fft(x-mean))\n",
    "  xf=scipy.fft.fftfreq(len(x),mean)[0:len(x)//2]\n",
    "  psd=(2/len(yf))*np.abs(yf)[0:len(x)//2]\n",
    "  fmean=np.mean(xf)\n",
    "  fstd=np.std(xf)\n",
    "  sumpsd=np.sum(psd)\n",
    "  return fmean,fstd,sumpsd\n",
    "\n",
    "# fmean,fstd,sumpsd=get_freq_features_ecg(hrv)\n",
    "# fmean,fstd, sumpsd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hrv_indices = nk.hrv(peaks, sampling_rate=1000)\n",
    "# hrv_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/Edouard99/Stress_Detection_ECG/tree/main\n",
    "# pd.DataFrame({\n",
    "#     'meanHR': meanHR,\n",
    "#     'stdHR': stdHR,\n",
    "#     'TINN': hrv_indices['HRV_TINN'],\n",
    "#     'HRVindex': HRVindex,\n",
    "#     '%NN50': num50,\n",
    "#     'pnn50': hrv_indices['HRV_pNN50'],\n",
    "#     'meanHRV': meanHRV,\n",
    "#     'stdHRV': stdHRV,\n",
    "#     'rmsHRV': rmsHRV,\n",
    "#     'Mean Fourier Frequencies': fmean,\n",
    "#     'STD Fourier Frequencies': fstd,\n",
    "#     'Sum PSD components': sumpsd\n",
    "# }, index=[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# frequencies = nk.hrv_frequency(\n",
    "#     peaks, \n",
    "#     sampling_rate=1000,\n",
    "#     ulf=[0.01,0.04],\n",
    "#     lf=[0.04,0.15],\n",
    "#     hf=[0.15,0.4],\n",
    "#     vhf=[0.4,1]\n",
    "# )\n",
    "# frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# total_power = np.nansum([frequencies['HRV_ULF'], frequencies['HRV_LF'], frequencies['HRV_HF'], frequencies['HRV_VHF']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://dl-acm-org.vu-nl.idm.oclc.org/doi/epdf/10.1145/3242969.3242985\n",
    "# pd.DataFrame({\n",
    "#     'μHR': meanHR,\n",
    "#     'σHR': stdHR,\n",
    "#     'μHRV': meanHRV,\n",
    "#     'σHRV': stdHRV,\n",
    "#     'NN50': num50, \n",
    "#     'pNN50': hrv_indices['HRV_pNN50'],\n",
    "#     'TINN': hrv_indices['HRV_TINN'],\n",
    "#     'rmsHRV': rmsHRV,\n",
    "#     'ULF': frequencies['HRV_ULF'],\n",
    "#     'LF': frequencies['HRV_LF'],\n",
    "#     'HF': frequencies['HRV_HF'],\n",
    "#     'UHF': frequencies['HRV_VHF'],\n",
    "#     'LF_HF_Ratio': frequencies['HRV_LF'] / frequencies['HRV_HF'],\n",
    "#     'total_power': total_power,\n",
    "#     'relative_power_ulf': (frequencies['HRV_ULF'] / total_power) * 100,\n",
    "#     'relative_power_lf': (frequencies['HRV_LF'] / total_power) * 100,\n",
    "#     'relative_power_hf': (frequencies['HRV_HF'] / total_power) * 100,\n",
    "#     'relative_power_vhf': (frequencies['HRV_VHF'] / total_power) * 100,\n",
    "#     'LF_norm': np.nan,  ## Can only be normalised after all the LF and HF are calculated\n",
    "#     'HF_norm': np.nan,  ## Can only be normalised after all the LF and HF are calculated\n",
    "# }, index=[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The “heart rate” can be described as a true rate in beats per minute (HR) or as the RR interval in milliseconds. \n",
    "The RR interval is the time elapsed between two successive R waves of the QRS signal on the electrocardiogram\n",
    "“Heart rate variability” has become the conventionally accepted term to describe variations of both instantaneous heart rate and RR intervals.\n",
    "\n",
    "The RR interval and HR are hyperbolically related (HR x RR interval = 60000; see figure 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hrv_indices.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from glob import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_window = 20 * 1000\n",
    "sampling_rate = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_shift_size = 1\n",
    "step_size = int(window_shift_size * sampling_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = glob('./data/ecg_model_with_features/*.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline = ['Sitting', 'Recov1', 'Recov2', 'Recov3', 'Recov4', 'Recov5', 'Recov6']\n",
    "mental_stress = ['TA', 'SSST_Sing_countdown', 'Pasat', 'Raven', 'TA_repeat', 'Pasat_repeat']\n",
    "high_physical_stress = ['Treadmill1', 'Treadmill2', 'Treadmill3', 'Treadmill4', 'Walking_fast_pace', 'Cycling', 'stairs_up_and_down']\n",
    "moderate_physical_stress = ['Walking_own_pace', 'Dishes', 'Vacuum']\n",
    "low_physical_stress = ['Standing', 'Lying_supine', 'Recov_standing']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_and_save(file):\n",
    "    dataset = load_dataset(\n",
    "        'csv', \n",
    "        data_files=file,\n",
    "        features=datasets.Features({\n",
    "            'signal': datasets.Value('float64'),\n",
    "            'label': datasets.Value('string'),\n",
    "        })\n",
    "    )['train']\n",
    "    neutral = dataset.filter(lambda x: x['label'] in baseline).select(range(90 * sampling_rate))\n",
    "    \n",
    "    neutral_peaks, _ = nk.ecg_peaks(neutral['signal'], sampling_rate=sampling_rate)\n",
    "    neutral_peaks_indices = neutral_peaks[neutral_peaks['ECG_R_Peaks'] == 1].index\n",
    "\n",
    "    ## HR\n",
    "    neutral_signal_rate = nk.signal_rate(neutral_peaks, sampling_rate=sampling_rate)\n",
    "    neutral_mean_hr = np.mean(neutral_signal_rate)\n",
    "    neutral_std_hr = np.std(neutral_signal_rate)\n",
    "\n",
    "    ## Frequencies\n",
    "    neutral_periods = np.array([(neutral_peaks_indices[i+1]-neutral_peaks_indices[i])/sampling_rate for i in range(0,len(neutral_peaks_indices)-1)])\n",
    "    neutral_frequency = 1 / neutral_periods\n",
    "    neutral_mean_freq = np.mean(neutral_frequency)\n",
    "    neutral_std_freq = np.std(neutral_frequency)\n",
    "    neutral_mean_f, neutral_std_f, neutral_sum_psd = get_freq_features_ecg(neutral_periods)\n",
    "    \n",
    "    ## HRV\n",
    "    neutral_hrv = np.array([(neutral_peaks_indices[i]-neutral_peaks_indices[i-1])/sampling_rate for i in range(1,len(neutral_peaks_indices))])\n",
    "    neutral_mean_hrv = np.mean(neutral_hrv)\n",
    "    neutral_std_hrv = np.std(neutral_hrv)\n",
    "    neutral_rms_hrv = np.sqrt(np.mean(neutral_hrv**2))\n",
    "    _, _, _, neutral_hrv_index = best_TINN(neutral_hrv)\n",
    "\n",
    "    ## %NN50\n",
    "    neutral_NN50, neutral_pNN50 = compare_NN50(neutral_hrv)\n",
    "\n",
    "    ## Power\n",
    "    neutral_frequencies = nk.hrv_frequency(\n",
    "        neutral_peaks, \n",
    "        sampling_rate=sampling_rate,\n",
    "        ulf=[0.01,0.04],\n",
    "        lf=[0.04,0.15],\n",
    "        hf=[0.15,0.4],\n",
    "        vhf=[0.4,1]\n",
    "    )\n",
    "    neutral_total_power = np.nansum([neutral_frequencies['HRV_ULF'], neutral_frequencies['HRV_LF'], neutral_frequencies['HRV_HF'], neutral_frequencies['HRV_VHF']])\n",
    "\n",
    "    dataframes = []\n",
    "    for start_idx in range(0, len(dataset), step_size): ## Window shift\n",
    "        try:\n",
    "            sample = dataset[start_idx:start_idx+n_window]\n",
    "            if len(sample['signal']) < n_window:\n",
    "                continue\n",
    "\n",
    "            signal = sample['signal']\n",
    "            label = collections.Counter(sample['label']).most_common(1)[0][0]\n",
    "\n",
    "            peaks, _ = nk.ecg_peaks(signal, sampling_rate=sampling_rate)\n",
    "            peaks_indices = peaks[peaks['ECG_R_Peaks'] == 1].index\n",
    "\n",
    "            ## HR\n",
    "            signal_rate = nk.signal_rate(peaks, sampling_rate=sampling_rate)\n",
    "            mean_hr = np.mean(signal_rate)\n",
    "            std_hr = np.std(signal_rate)\n",
    "\n",
    "            ## Frequencies\n",
    "            periods = np.array([(peaks_indices[i+1]-peaks_indices[i])/sampling_rate for i in range(0,len(peaks_indices)-1)])\n",
    "            frequency = 1 / periods\n",
    "            mean_freq = np.mean(frequency)\n",
    "            std_freq = np.std(frequency)\n",
    "            mean_f, std_f, sum_psd = get_freq_features_ecg(periods)\n",
    "            \n",
    "            ## HRV\n",
    "            hrv = np.array([(peaks_indices[i]-peaks_indices[i-1])/sampling_rate for i in range(1,len(peaks_indices))])\n",
    "            mean_hrv = np.mean(hrv)\n",
    "            std_hrv = np.std(hrv)\n",
    "            rms_hrv = np.sqrt(np.mean(hrv**2))\n",
    "            _, _, _, hrv_index = best_TINN(hrv)\n",
    "\n",
    "            ## %NN50\n",
    "            NN50, pNN50 = compare_NN50(hrv)\n",
    "\n",
    "            ## Power\n",
    "            frequencies = nk.hrv_frequency(\n",
    "                peaks, \n",
    "                sampling_rate=sampling_rate,\n",
    "                ulf=[0.01,0.04],\n",
    "                lf=[0.04,0.15],\n",
    "                hf=[0.15,0.4],\n",
    "                vhf=[0.4,1]\n",
    "            )\n",
    "            total_power = np.nansum([frequencies['HRV_ULF'], frequencies['HRV_LF'], frequencies['HRV_HF'], frequencies['HRV_VHF']])\n",
    "\n",
    "            ## Dataframe\n",
    "            df = nk.hrv(peaks, sampling_rate=sampling_rate)\n",
    "            df['label'] = label\n",
    "            df['mean_hr'] = mean_hr / neutral_mean_hr\n",
    "            df['std_hr'] = std_hr / neutral_std_hr\n",
    "            df['hrv_index'] = hrv_index / neutral_hrv_index\n",
    "            df['nn50'] = NN50 / neutral_NN50\n",
    "            df['mean_hrv'] = mean_hrv / neutral_mean_hrv\n",
    "            df['std_hrv'] = std_hrv / neutral_std_hrv\n",
    "            df['rms_hrv'] = rms_hrv / neutral_rms_hrv\n",
    "            df['mean_fourier_frequencies'] = mean_f / neutral_mean_f\n",
    "            df['std_fourier_frequencies'] = std_f / neutral_std_f\n",
    "            df['sum_psd'] = sum_psd / neutral_sum_psd\n",
    "            df['ulf'] = frequencies['HRV_ULF'] / neutral_frequencies['HRV_ULF']\n",
    "            df['lf'] = frequencies['HRV_LF'] / neutral_frequencies['HRV_LF']\n",
    "            df['hf'] = frequencies['HRV_HF'] / neutral_frequencies['HRV_HF']\n",
    "            df['uhf'] = frequencies['HRV_VHF'] / neutral_frequencies['HRV_VHF']\n",
    "            df['lf_hf_ratio'] = (frequencies['HRV_LF'] / frequencies['HRV_HF']) / (neutral_frequencies['HRV_LF'] / neutral_frequencies['HRV_HF'])\n",
    "            df['total_power'] = total_power / neutral_total_power\n",
    "            df['relative_power_ulf'] = ((frequencies['HRV_ULF'] / total_power) * 100) / ((neutral_frequencies['HRV_ULF'] / neutral_total_power) * 100)\n",
    "            df['relative_power_lf'] = ((frequencies['HRV_LF'] / total_power) * 100) / ((neutral_frequencies['HRV_LF'] / neutral_total_power) * 100)\n",
    "            df['relative_power_hf'] = ((frequencies['HRV_HF'] / total_power) * 100) / ((neutral_frequencies['HRV_HF'] / neutral_total_power) * 100)\n",
    "            df['relative_power_uhf'] = ((frequencies['HRV_VHF'] / total_power) * 100) / ((neutral_frequencies['HRV_VHF'] / neutral_total_power) * 100)\n",
    "            dataframes.append(df)\n",
    "        except Exception as e:\n",
    "            # print(e)\n",
    "            continue\n",
    "        \n",
    "    result = pd.concat(dataframes, ignore_index=True)\n",
    "\n",
    "    stem = Path(file).stem\n",
    "    result.to_csv(f'./data/ecg_features_20s/{stem}.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import Parallel, delayed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None]"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Parallel(n_jobs=6)(delayed(preprocess_and_save)(file) for file in files) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
