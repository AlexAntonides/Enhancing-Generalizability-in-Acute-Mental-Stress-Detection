{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sia import Pipeline\n",
    "from sia.io import Metadata, read_edf, read_csv, read_dataset, write_csv, write_png, write_dataset\n",
    "from sia.preprocessors import neurokit\n",
    "from sia.transformers import as_window, scaleogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_category(target):\n",
    "    def inner(label):\n",
    "        return {\n",
    "            'label': np.isin(label, target).astype(int)\n",
    "        }\n",
    "    return inner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize(width: int, height: int):\n",
    "    def inner(image):\n",
    "        return {\n",
    "            'pixel_values': [x.convert(\"RGB\").resize((width, height)) for x in image]\n",
    "        }\n",
    "    return inner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99723053f5c94a1eb68f7129621f704c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=8):   0%|          | 0/8692000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c25edcac22c2489483005aa2548f5413",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter (num_proc=8):   0%|          | 0/8692000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50b007df581142c284a631311502ccfe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating CSV from Arrow format:   0%|          | 0/8671 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9fa4bdb8aff4934b4dd7d0e7509bdbc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=8):   0%|          | 0/8344000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 10\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[43mPipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m        \u001b[49m\u001b[43mread_edf\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m./data/ecg_raw/*.edf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m            \u001b[49m\u001b[43mMetadata\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m./data/ecg_raw/TimeStamps_Merged.txt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mon_regex\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m[0-9]\u001b[39;49m\u001b[38;5;132;43;01m{5}\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43mneurokit\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfilter\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mECG_Quality\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mquality\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m>\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m.25\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mquality\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mECG_Quality\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m---> 10\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwrite_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m./data/ecg_preprocessed_peaks/[0-9]\u001b[39;49m\u001b[38;5;132;43;01m{5}\u001b[39;49;00m\u001b[38;5;124;43m.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\university\\thesis_old\\sia\\builders\\pipeline.py:85\u001b[0m, in \u001b[0;36mPipeline.to\u001b[1;34m(self, writer)\u001b[0m\n\u001b[0;32m     84\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mto\u001b[39m(\u001b[38;5;28mself\u001b[39m, writer: Callable[[Union[Dataset, IterableDataset]], Union[Dataset, IterableDataset]]):\n\u001b[1;32m---> 85\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m path, dataset \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterate():\n\u001b[0;32m     86\u001b[0m         filename \u001b[38;5;241m=\u001b[39m Path(path)\u001b[38;5;241m.\u001b[39mstem\n\u001b[0;32m     87\u001b[0m         writer(filename, dataset)\n",
      "File \u001b[1;32md:\\university\\thesis_old\\sia\\builders\\pipeline.py:105\u001b[0m, in \u001b[0;36mPipeline._iterate\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    103\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m path, dataset \u001b[38;5;129;01min\u001b[39;00m value():\n\u001b[0;32m    104\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msettings[index\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m:]) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m--> 105\u001b[0m             dataset \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msettings\u001b[49m\u001b[43m[\u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    107\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m path, dataset\n\u001b[0;32m    108\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m: \n",
      "File \u001b[1;32md:\\university\\thesis_old\\sia\\builders\\pipeline.py:118\u001b[0m, in \u001b[0;36mPipeline._execute\u001b[1;34m(self, dataset, setting)\u001b[0m\n\u001b[0;32m    116\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(setting, \u001b[38;5;28mlist\u001b[39m):\n\u001b[0;32m    117\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m setting:\n\u001b[1;32m--> 118\u001b[0m         dataset \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    119\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m setting\u001b[38;5;241m.\u001b[39mitems():\n",
      "File \u001b[1;32md:\\university\\thesis_old\\sia\\builders\\pipeline.py:130\u001b[0m, in \u001b[0;36mPipeline._execute\u001b[1;34m(self, dataset, setting)\u001b[0m\n\u001b[0;32m    127\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdataset\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m args:\n\u001b[0;32m    128\u001b[0m         kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdataset\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m dataset\n\u001b[1;32m--> 130\u001b[0m     dataset \u001b[38;5;241m=\u001b[39m \u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    131\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    132\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatched\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    133\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m.25\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    134\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_columns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mintersection\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    135\u001b[0m \u001b[43m        \u001b[49m\u001b[43mwith_indices\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43midxs\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    136\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfn_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    137\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_proc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m8\u001b[39;49m\n\u001b[0;32m    138\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    139\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m key \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mselect\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m    140\u001b[0m     dataset \u001b[38;5;241m=\u001b[39m dataset\u001b[38;5;241m.\u001b[39mselect_columns(value)\n",
      "File \u001b[1;32mc:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\datasets\\arrow_dataset.py:593\u001b[0m, in \u001b[0;36mtransmit_tasks.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    591\u001b[0m     \u001b[38;5;28mself\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mself\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    592\u001b[0m \u001b[38;5;66;03m# apply actual function\u001b[39;00m\n\u001b[1;32m--> 593\u001b[0m out: Union[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDatasetDict\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    594\u001b[0m datasets: List[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(out\u001b[38;5;241m.\u001b[39mvalues()) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(out, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m [out]\n\u001b[0;32m    595\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m dataset \u001b[38;5;129;01min\u001b[39;00m datasets:\n\u001b[0;32m    596\u001b[0m     \u001b[38;5;66;03m# Remove task templates if a column mapping of the template is no longer valid\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\datasets\\arrow_dataset.py:558\u001b[0m, in \u001b[0;36mtransmit_format.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    551\u001b[0m self_format \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    552\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_type,\n\u001b[0;32m    553\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mformat_kwargs\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_kwargs,\n\u001b[0;32m    554\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_columns,\n\u001b[0;32m    555\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_all_columns\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output_all_columns,\n\u001b[0;32m    556\u001b[0m }\n\u001b[0;32m    557\u001b[0m \u001b[38;5;66;03m# apply actual function\u001b[39;00m\n\u001b[1;32m--> 558\u001b[0m out: Union[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDatasetDict\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    559\u001b[0m datasets: List[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(out\u001b[38;5;241m.\u001b[39mvalues()) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(out, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m [out]\n\u001b[0;32m    560\u001b[0m \u001b[38;5;66;03m# re-apply format to the output\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\datasets\\arrow_dataset.py:3197\u001b[0m, in \u001b[0;36mDataset.map\u001b[1;34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, suffix_template, new_fingerprint, desc)\u001b[0m\n\u001b[0;32m   3191\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSpawning \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_proc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m processes\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   3192\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m hf_tqdm(\n\u001b[0;32m   3193\u001b[0m     unit\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m examples\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   3194\u001b[0m     total\u001b[38;5;241m=\u001b[39mpbar_total,\n\u001b[0;32m   3195\u001b[0m     desc\u001b[38;5;241m=\u001b[39m(desc \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMap\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m (num_proc=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_proc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   3196\u001b[0m ) \u001b[38;5;28;01mas\u001b[39;00m pbar:\n\u001b[1;32m-> 3197\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m rank, done, content \u001b[38;5;129;01min\u001b[39;00m iflatmap_unordered(\n\u001b[0;32m   3198\u001b[0m         pool, Dataset\u001b[38;5;241m.\u001b[39m_map_single, kwargs_iterable\u001b[38;5;241m=\u001b[39mkwargs_per_job\n\u001b[0;32m   3199\u001b[0m     ):\n\u001b[0;32m   3200\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m done:\n\u001b[0;32m   3201\u001b[0m             shards_done \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\datasets\\utils\\py_utils.py:651\u001b[0m, in \u001b[0;36miflatmap_unordered\u001b[1;34m(pool, func, kwargs_iterable)\u001b[0m\n\u001b[0;32m    649\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m    650\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 651\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m \u001b[43mqueue\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.05\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    652\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Empty:\n\u001b[0;32m    653\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mall\u001b[39m(async_result\u001b[38;5;241m.\u001b[39mready() \u001b[38;5;28;01mfor\u001b[39;00m async_result \u001b[38;5;129;01min\u001b[39;00m async_results) \u001b[38;5;129;01mand\u001b[39;00m queue\u001b[38;5;241m.\u001b[39mempty():\n",
      "File \u001b[1;32m<string>:2\u001b[0m, in \u001b[0;36mget\u001b[1;34m(self, *args, **kwds)\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\multiprocess\\managers.py:822\u001b[0m, in \u001b[0;36mBaseProxy._callmethod\u001b[1;34m(self, methodname, args, kwds)\u001b[0m\n\u001b[0;32m    819\u001b[0m     conn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tls\u001b[38;5;241m.\u001b[39mconnection\n\u001b[0;32m    821\u001b[0m conn\u001b[38;5;241m.\u001b[39msend((\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_id, methodname, args, kwds))\n\u001b[1;32m--> 822\u001b[0m kind, result \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    824\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m kind \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m#RETURN\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m    825\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[1;32mc:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\multiprocess\\connection.py:254\u001b[0m, in \u001b[0;36m_ConnectionBase.recv\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    252\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_readable()\n\u001b[0;32m    253\u001b[0m buf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_recv_bytes()\n\u001b[1;32m--> 254\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_ForkingPickler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloads\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbuf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetbuffer\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\dill\\_dill.py:303\u001b[0m, in \u001b[0;36mloads\u001b[1;34m(str, ignore, **kwds)\u001b[0m\n\u001b[0;32m    292\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    293\u001b[0m \u001b[38;5;124;03mUnpickle an object from a string.\u001b[39;00m\n\u001b[0;32m    294\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    300\u001b[0m \u001b[38;5;124;03mDefault values for keyword arguments can be set in :mod:`dill.settings`.\u001b[39;00m\n\u001b[0;32m    301\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    302\u001b[0m file \u001b[38;5;241m=\u001b[39m StringIO(\u001b[38;5;28mstr\u001b[39m)\n\u001b[1;32m--> 303\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\dill\\_dill.py:289\u001b[0m, in \u001b[0;36mload\u001b[1;34m(file, ignore, **kwds)\u001b[0m\n\u001b[0;32m    283\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload\u001b[39m(file, ignore\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds):\n\u001b[0;32m    284\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    285\u001b[0m \u001b[38;5;124;03m    Unpickle an object from a file.\u001b[39;00m\n\u001b[0;32m    286\u001b[0m \n\u001b[0;32m    287\u001b[0m \u001b[38;5;124;03m    See :func:`loads` for keyword arguments.\u001b[39;00m\n\u001b[0;32m    288\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 289\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mUnpickler\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\dill\\_dill.py:444\u001b[0m, in \u001b[0;36mUnpickler.load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    443\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload\u001b[39m(\u001b[38;5;28mself\u001b[39m): \u001b[38;5;66;03m#NOTE: if settings change, need to update attributes\u001b[39;00m\n\u001b[1;32m--> 444\u001b[0m     obj \u001b[38;5;241m=\u001b[39m \u001b[43mStockUnpickler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    445\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(obj)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__module__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(_main_module, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__name__\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m    446\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ignore:\n\u001b[0;32m    447\u001b[0m             \u001b[38;5;66;03m# point obj class to main\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pyarrow\\types.pxi:4909\u001b[0m, in \u001b[0;36mpyarrow.lib.schema\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m<frozen abc>:117\u001b[0m, in \u001b[0;36m__instancecheck__\u001b[1;34m(cls, instance)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "Pipeline() \\\n",
    "    .data(\n",
    "        read_edf(\n",
    "            './data/ecg_raw/*.edf', \n",
    "            Metadata('./data/ecg_raw/TimeStamps_Merged.txt').on_regex(r'[0-9]{5}')\n",
    "        )\n",
    "    ) \\\n",
    "    .process(neurokit()) \\\n",
    "    .filter(lambda ECG_Quality: [quality > .25 for quality in ECG_Quality]) \\\n",
    "    .to(write_csv('./data/ecg_preprocessed_peaks/[0-9]{5}.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 8692000 examples [00:15, 555522.88 examples/s]\n",
      "Creating CSV from Arrow format: 100%|██████████| 8692/8692 [00:15<00:00, 568.14ba/s]\n",
      "Generating train split: 8344000 examples [00:14, 565807.32 examples/s]\n",
      "Creating CSV from Arrow format: 100%|██████████| 8344/8344 [00:14<00:00, 581.78ba/s]\n",
      "Generating train split: 8174000 examples [00:15, 535978.88 examples/s]\n",
      "Creating CSV from Arrow format: 100%|██████████| 8174/8174 [00:14<00:00, 569.86ba/s]\n",
      "Generating train split: 9684000 examples [00:18, 518489.69 examples/s]\n",
      "Creating CSV from Arrow format: 100%|██████████| 9684/9684 [00:16<00:00, 574.73ba/s]\n",
      "Generating train split: 7842000 examples [00:14, 529285.42 examples/s]\n",
      "Creating CSV from Arrow format: 100%|██████████| 7842/7842 [00:13<00:00, 563.19ba/s]\n",
      "Generating train split: 7729000 examples [00:15, 509793.63 examples/s]\n",
      "Creating CSV from Arrow format: 100%|██████████| 7729/7729 [00:13<00:00, 553.39ba/s]\n",
      "Generating train split: 7737000 examples [00:14, 521745.65 examples/s]\n",
      "Creating CSV from Arrow format: 100%|██████████| 7737/7737 [00:13<00:00, 575.60ba/s]\n",
      "Generating train split: 9241000 examples [00:17, 523509.20 examples/s]\n",
      "Creating CSV from Arrow format: 100%|██████████| 9241/9241 [00:16<00:00, 558.26ba/s]\n",
      "Generating train split: 8718000 examples [00:16, 521031.60 examples/s]\n",
      "Creating CSV from Arrow format: 100%|██████████| 8718/8718 [00:15<00:00, 564.55ba/s]\n",
      "Generating train split: 10272000 examples [00:19, 524718.62 examples/s]\n",
      "Creating CSV from Arrow format: 100%|██████████| 10272/10272 [00:17<00:00, 579.72ba/s]\n",
      "Generating train split: 8212000 examples [00:15, 531831.37 examples/s]\n",
      "Creating CSV from Arrow format: 100%|██████████| 8212/8212 [00:14<00:00, 565.53ba/s]\n",
      "Generating train split: 7871000 examples [00:14, 559659.41 examples/s]\n",
      "Creating CSV from Arrow format: 100%|██████████| 7871/7871 [00:13<00:00, 565.51ba/s]\n",
      "Generating train split: 8530000 examples [00:16, 532620.55 examples/s]\n",
      "Creating CSV from Arrow format: 100%|██████████| 8530/8530 [00:15<00:00, 563.40ba/s]\n",
      "Generating train split: 7112000 examples [00:13, 534840.13 examples/s]\n",
      "Creating CSV from Arrow format: 100%|██████████| 7112/7112 [00:12<00:00, 555.32ba/s]\n",
      "Generating train split: 8624000 examples [00:15, 542479.79 examples/s]\n",
      "Creating CSV from Arrow format: 100%|██████████| 8624/8624 [00:15<00:00, 556.21ba/s]\n",
      "Generating train split: 9975000 examples [00:18, 544313.89 examples/s]\n",
      "Creating CSV from Arrow format: 100%|██████████| 9975/9975 [00:17<00:00, 572.17ba/s]\n",
      "Generating train split: 8185000 examples [00:15, 521571.76 examples/s]\n",
      "Creating CSV from Arrow format: 100%|██████████| 8185/8185 [00:14<00:00, 568.82ba/s]\n",
      "Generating train split: 8849000 examples [00:16, 526619.84 examples/s]\n",
      "Creating CSV from Arrow format: 100%|██████████| 8849/8849 [00:15<00:00, 561.59ba/s]\n",
      "Generating train split: 7552000 examples [00:14, 525958.42 examples/s]\n",
      "Creating CSV from Arrow format: 100%|██████████| 7552/7552 [00:13<00:00, 570.93ba/s]\n",
      "Generating train split: 7297000 examples [00:14, 515573.37 examples/s]\n",
      "Creating CSV from Arrow format: 100%|██████████| 7297/7297 [00:13<00:00, 554.03ba/s]\n",
      "Generating train split: 7482000 examples [00:14, 510342.66 examples/s]\n",
      "Creating CSV from Arrow format: 100%|██████████| 7482/7482 [00:13<00:00, 574.91ba/s]\n",
      "Generating train split: 7726000 examples [00:14, 527474.93 examples/s]\n",
      "Creating CSV from Arrow format: 100%|██████████| 7726/7726 [00:13<00:00, 560.92ba/s]\n",
      "Generating train split: 8568000 examples [00:15, 547401.82 examples/s]\n",
      "Creating CSV from Arrow format: 100%|██████████| 8568/8568 [00:15<00:00, 543.41ba/s]\n",
      "Generating train split: 7769000 examples [00:15, 509635.77 examples/s]\n",
      "Creating CSV from Arrow format: 100%|██████████| 7769/7769 [00:13<00:00, 560.91ba/s]\n",
      "Generating train split: 8439000 examples [00:15, 531026.73 examples/s]\n",
      "Creating CSV from Arrow format: 100%|██████████| 8439/8439 [00:15<00:00, 552.07ba/s]\n",
      "Generating train split: 7259000 examples [00:14, 515038.97 examples/s]\n",
      "Creating CSV from Arrow format: 100%|██████████| 7259/7259 [00:12<00:00, 564.79ba/s]\n",
      "Generating train split: 8211000 examples [00:15, 520978.11 examples/s]\n",
      "Creating CSV from Arrow format: 100%|██████████| 8211/8211 [00:14<00:00, 548.44ba/s]\n",
      "Generating train split: 7726000 examples [00:15, 513226.27 examples/s]\n",
      "Creating CSV from Arrow format: 100%|██████████| 7726/7726 [00:13<00:00, 570.71ba/s]\n",
      "Generating train split: 7106000 examples [00:13, 519548.12 examples/s]\n",
      "Creating CSV from Arrow format: 100%|██████████| 7106/7106 [00:12<00:00, 547.18ba/s]\n",
      "Generating train split: 7875000 examples [00:15, 517548.01 examples/s]\n",
      "Creating CSV from Arrow format: 100%|██████████| 7875/7875 [00:13<00:00, 565.07ba/s]\n",
      "Generating train split: 7772000 examples [00:14, 526557.27 examples/s]\n",
      "Creating CSV from Arrow format: 100%|██████████| 7772/7772 [00:13<00:00, 557.86ba/s]\n",
      "Generating train split: 7589000 examples [00:15, 500585.52 examples/s]\n",
      "Creating CSV from Arrow format: 100%|██████████| 7589/7589 [00:13<00:00, 563.86ba/s]\n",
      "Generating train split: 8139000 examples [00:15, 527912.13 examples/s]\n",
      "Creating CSV from Arrow format: 100%|██████████| 8139/8139 [00:14<00:00, 556.41ba/s]\n",
      "Generating train split: 7324000 examples [00:13, 554435.36 examples/s]\n",
      "Creating CSV from Arrow format: 100%|██████████| 7324/7324 [00:12<00:00, 567.94ba/s]\n",
      "Generating train split: 7880000 examples [00:14, 530882.75 examples/s]\n",
      "Creating CSV from Arrow format: 100%|██████████| 7880/7880 [00:13<00:00, 566.98ba/s]\n",
      "Generating train split: 7873000 examples [00:14, 532685.62 examples/s]\n",
      "Creating CSV from Arrow format: 100%|██████████| 7873/7873 [00:13<00:00, 567.53ba/s]\n",
      "Generating train split: 7462000 examples [00:13, 534462.19 examples/s]\n",
      "Creating CSV from Arrow format: 100%|██████████| 7462/7462 [00:13<00:00, 557.29ba/s]\n",
      "Generating train split: 8370000 examples [00:15, 526036.75 examples/s]\n",
      "Creating CSV from Arrow format: 100%|██████████| 8370/8370 [00:14<00:00, 567.24ba/s]\n",
      "Generating train split: 6906000 examples [00:13, 525565.18 examples/s]\n",
      "Creating CSV from Arrow format: 100%|██████████| 6906/6906 [00:12<00:00, 562.60ba/s]\n",
      "Generating train split: 7847000 examples [00:14, 523495.50 examples/s]\n",
      "Creating CSV from Arrow format: 100%|██████████| 7847/7847 [00:14<00:00, 558.85ba/s]\n",
      "Generating train split: 7861000 examples [00:15, 522723.42 examples/s]\n",
      "Creating CSV from Arrow format: 100%|██████████| 7861/7861 [00:14<00:00, 561.18ba/s]\n",
      "Generating train split: 7451000 examples [00:14, 506160.69 examples/s]\n",
      "Creating CSV from Arrow format: 100%|██████████| 7451/7451 [00:13<00:00, 549.58ba/s]\n",
      "Generating train split: 8050000 examples [00:15, 522485.15 examples/s]\n",
      "Creating CSV from Arrow format: 100%|██████████| 8050/8050 [00:14<00:00, 564.07ba/s]\n",
      "Generating train split: 8025000 examples [00:15, 502602.55 examples/s]\n",
      "Creating CSV from Arrow format: 100%|██████████| 8025/8025 [00:14<00:00, 550.59ba/s]\n",
      "Generating train split: 8035000 examples [00:14, 557603.28 examples/s]\n",
      "Creating CSV from Arrow format: 100%|██████████| 8035/8035 [00:14<00:00, 572.73ba/s]\n",
      "Generating train split: 8123000 examples [00:15, 512893.68 examples/s]\n",
      "Creating CSV from Arrow format: 100%|██████████| 8123/8123 [00:14<00:00, 553.18ba/s]\n",
      "Generating train split: 8309000 examples [00:16, 516162.40 examples/s]\n",
      "Creating CSV from Arrow format: 100%|██████████| 8309/8309 [00:15<00:00, 553.29ba/s]\n",
      "Generating train split: 7275000 examples [00:14, 508955.95 examples/s]\n",
      "Creating CSV from Arrow format: 100%|██████████| 7275/7275 [00:13<00:00, 552.86ba/s]\n",
      "Generating train split: 7870000 examples [00:15, 522620.61 examples/s]\n",
      "Creating CSV from Arrow format: 100%|██████████| 7870/7870 [00:13<00:00, 565.80ba/s]\n",
      "Generating train split: 6866000 examples [00:13, 522261.63 examples/s]\n",
      "Creating CSV from Arrow format: 100%|██████████| 6866/6866 [00:12<00:00, 557.86ba/s]\n",
      "Generating train split: 7985000 examples [00:15, 524638.33 examples/s]\n",
      "Creating CSV from Arrow format: 100%|██████████| 7985/7985 [00:14<00:00, 562.82ba/s]\n",
      "Generating train split: 7835000 examples [00:14, 523144.29 examples/s]\n",
      "Creating CSV from Arrow format: 100%|██████████| 7835/7835 [00:13<00:00, 569.07ba/s]\n",
      "Generating train split: 7633000 examples [00:15, 501394.03 examples/s]\n",
      "Creating CSV from Arrow format: 100%|██████████| 7633/7633 [00:13<00:00, 565.26ba/s]\n",
      "Generating train split: 7805000 examples [00:14, 520911.88 examples/s]\n",
      "Creating CSV from Arrow format: 100%|██████████| 7805/7805 [00:13<00:00, 559.54ba/s]\n",
      "Generating train split: 6100000 examples [00:12, 502979.20 examples/s]\n",
      "Creating CSV from Arrow format: 100%|██████████| 6100/6100 [00:10<00:00, 558.03ba/s]\n",
      "Generating train split: 7232000 examples [00:13, 551959.45 examples/s]\n",
      "Creating CSV from Arrow format: 100%|██████████| 7232/7232 [00:13<00:00, 549.55ba/s]\n",
      "Generating train split: 8089000 examples [00:15, 521840.03 examples/s]\n",
      "Creating CSV from Arrow format: 100%|██████████| 8089/8089 [00:14<00:00, 564.05ba/s]\n",
      "Generating train split: 7762000 examples [00:14, 530207.33 examples/s]\n",
      "Creating CSV from Arrow format: 100%|██████████| 7762/7762 [00:13<00:00, 556.22ba/s]\n",
      "Generating train split: 7241000 examples [00:13, 522486.78 examples/s]\n",
      "Creating CSV from Arrow format: 100%|██████████| 7241/7241 [00:12<00:00, 558.43ba/s]\n",
      "Generating train split: 7997000 examples [00:15, 525456.96 examples/s]\n",
      "Creating CSV from Arrow format: 100%|██████████| 7997/7997 [00:14<00:00, 559.37ba/s]\n",
      "Generating train split: 7050000 examples [00:13, 519509.02 examples/s]\n",
      "Creating CSV from Arrow format: 100%|██████████| 7050/7050 [00:12<00:00, 564.78ba/s]\n",
      "Generating train split: 7238000 examples [00:13, 523608.31 examples/s]\n",
      "Creating CSV from Arrow format: 100%|██████████| 7238/7238 [00:13<00:00, 553.19ba/s]\n",
      "Generating train split: 7012000 examples [00:13, 518769.17 examples/s]\n",
      "Creating CSV from Arrow format: 100%|██████████| 7012/7012 [00:12<00:00, 558.22ba/s]\n",
      "Generating train split: 8143000 examples [00:15, 514888.62 examples/s]\n",
      "Creating CSV from Arrow format: 100%|██████████| 8143/8143 [00:14<00:00, 551.07ba/s]\n",
      "Generating train split: 7419000 examples [00:14, 500125.92 examples/s]\n",
      "Creating CSV from Arrow format: 100%|██████████| 7419/7419 [00:13<00:00, 558.29ba/s]\n",
      "Generating train split: 7902000 examples [00:15, 499454.74 examples/s]\n",
      "Creating CSV from Arrow format: 100%|██████████| 7902/7902 [00:13<00:00, 569.69ba/s]\n",
      "Generating train split: 7325000 examples [00:14, 517729.52 examples/s]\n",
      "Creating CSV from Arrow format: 100%|██████████| 7325/7325 [00:13<00:00, 554.05ba/s]\n",
      "Generating train split: 8207000 examples [00:15, 540785.80 examples/s]\n",
      "Creating CSV from Arrow format: 100%|██████████| 8207/8207 [00:14<00:00, 555.94ba/s]\n",
      "Generating train split: 7823000 examples [00:14, 524892.22 examples/s]\n",
      "Creating CSV from Arrow format: 100%|██████████| 7823/7823 [00:13<00:00, 562.86ba/s]\n",
      "Generating train split: 7397000 examples [00:14, 512720.02 examples/s]\n",
      "Creating CSV from Arrow format: 100%|██████████| 7397/7397 [00:13<00:00, 563.30ba/s]\n",
      "Generating train split: 7013000 examples [00:13, 521498.88 examples/s]\n",
      "Creating CSV from Arrow format: 100%|██████████| 7013/7013 [00:12<00:00, 564.54ba/s]\n",
      "Generating train split: 7347000 examples [00:14, 517594.26 examples/s]\n",
      "Creating CSV from Arrow format: 100%|██████████| 7347/7347 [00:12<00:00, 569.70ba/s]\n",
      "Generating train split: 6959000 examples [00:13, 524872.62 examples/s]\n",
      "Creating CSV from Arrow format: 100%|██████████| 6959/6959 [00:12<00:00, 562.90ba/s]\n",
      "Generating train split: 8164000 examples [00:15, 524696.39 examples/s]\n",
      "Creating CSV from Arrow format: 100%|██████████| 8164/8164 [00:14<00:00, 559.86ba/s]\n",
      "Generating train split: 6658000 examples [00:13, 507862.81 examples/s]\n",
      "Creating CSV from Arrow format: 100%|██████████| 6658/6658 [00:12<00:00, 550.92ba/s]\n",
      "Generating train split: 7479000 examples [00:14, 519612.07 examples/s]\n",
      "Creating CSV from Arrow format: 100%|██████████| 7479/7479 [00:13<00:00, 555.84ba/s]\n",
      "Generating train split: 7982000 examples [00:15, 508525.65 examples/s]\n",
      "Creating CSV from Arrow format: 100%|██████████| 7982/7982 [00:14<00:00, 563.80ba/s]\n",
      "Generating train split: 7542000 examples [00:14, 512957.82 examples/s]\n",
      "Creating CSV from Arrow format: 100%|██████████| 7542/7542 [00:13<00:00, 547.62ba/s]\n",
      "Generating train split: 7413000 examples [00:13, 559805.85 examples/s]\n",
      "Creating CSV from Arrow format: 100%|██████████| 7413/7413 [00:13<00:00, 551.10ba/s]\n",
      "Generating train split: 7744000 examples [00:14, 522463.08 examples/s]\n",
      "Creating CSV from Arrow format: 100%|██████████| 7744/7744 [00:13<00:00, 561.75ba/s]\n",
      "Generating train split: 7381000 examples [00:14, 525993.59 examples/s]\n",
      "Creating CSV from Arrow format: 100%|██████████| 7381/7381 [00:12<00:00, 569.32ba/s]\n",
      "Generating train split: 7376000 examples [00:14, 523507.16 examples/s]\n",
      "Creating CSV from Arrow format: 100%|██████████| 7376/7376 [00:13<00:00, 553.91ba/s]\n",
      "Generating train split: 8129000 examples [00:15, 527686.25 examples/s]\n",
      "Creating CSV from Arrow format: 100%|██████████| 8129/8129 [00:15<00:00, 539.47ba/s]\n",
      "Generating train split: 7346000 examples [00:14, 523118.07 examples/s]\n",
      "Creating CSV from Arrow format: 100%|██████████| 7346/7346 [00:13<00:00, 557.48ba/s]\n",
      "Generating train split: 6913000 examples [00:13, 517075.63 examples/s]\n",
      "Creating CSV from Arrow format: 100%|██████████| 6913/6913 [00:12<00:00, 558.70ba/s]\n",
      "Generating train split: 6830000 examples [00:13, 522593.03 examples/s]\n",
      "Creating CSV from Arrow format: 100%|██████████| 6830/6830 [00:12<00:00, 560.36ba/s]\n",
      "Generating train split: 7656000 examples [00:15, 500525.12 examples/s]\n",
      "Creating CSV from Arrow format: 100%|██████████| 7656/7656 [00:14<00:00, 544.72ba/s]\n",
      "Generating train split: 7192000 examples [00:14, 491477.96 examples/s]\n",
      "Creating CSV from Arrow format: 100%|██████████| 7192/7192 [00:13<00:00, 548.04ba/s]\n",
      "Generating train split: 7583000 examples [00:15, 497545.24 examples/s]\n",
      "Creating CSV from Arrow format: 100%|██████████| 7583/7583 [00:13<00:00, 564.74ba/s]\n",
      "Generating train split: 7588000 examples [00:14, 533632.31 examples/s]\n",
      "Creating CSV from Arrow format: 100%|██████████| 7588/7588 [00:13<00:00, 557.79ba/s]\n",
      "Generating train split: 7237000 examples [00:13, 534118.32 examples/s]\n",
      "Creating CSV from Arrow format: 100%|██████████| 7237/7237 [00:13<00:00, 554.42ba/s]\n",
      "Generating train split: 6941000 examples [00:13, 518940.72 examples/s]\n",
      "Creating CSV from Arrow format: 100%|██████████| 6941/6941 [00:12<00:00, 550.15ba/s]\n",
      "Generating train split: 7337000 examples [00:14, 518757.31 examples/s]\n",
      "Creating CSV from Arrow format: 100%|██████████| 7337/7337 [00:13<00:00, 557.38ba/s]\n",
      "Generating train split: 7641000 examples [00:14, 521535.30 examples/s]\n",
      "Creating CSV from Arrow format: 100%|██████████| 7641/7641 [00:13<00:00, 559.97ba/s]\n",
      "Generating train split: 7662000 examples [00:14, 518752.52 examples/s]\n",
      "Creating CSV from Arrow format: 100%|██████████| 7662/7662 [00:13<00:00, 554.85ba/s]\n",
      "Generating train split: 7176000 examples [00:13, 520809.42 examples/s]\n",
      "Creating CSV from Arrow format: 100%|██████████| 7176/7176 [00:12<00:00, 553.44ba/s]\n",
      "Generating train split: 7023000 examples [00:13, 504785.33 examples/s]\n",
      "Creating CSV from Arrow format: 100%|██████████| 7023/7023 [00:12<00:00, 560.07ba/s]\n",
      "Generating train split: 7087000 examples [00:13, 516805.53 examples/s]\n",
      "Creating CSV from Arrow format: 100%|██████████| 7087/7087 [00:12<00:00, 558.46ba/s]\n",
      "Generating train split: 7326000 examples [00:14, 509203.35 examples/s]\n",
      "Creating CSV from Arrow format: 100%|██████████| 7326/7326 [00:13<00:00, 542.14ba/s]\n",
      "Generating train split: 7063000 examples [00:13, 514500.78 examples/s]\n",
      "Creating CSV from Arrow format: 100%|██████████| 7063/7063 [00:13<00:00, 542.81ba/s]\n",
      "Generating train split: 7312000 examples [00:15, 486523.40 examples/s]\n",
      "Creating CSV from Arrow format: 100%|██████████| 7312/7312 [00:13<00:00, 537.85ba/s]\n",
      "Generating train split: 7828000 examples [00:14, 542076.63 examples/s]\n",
      "Creating CSV from Arrow format: 100%|██████████| 7828/7828 [00:14<00:00, 557.57ba/s]\n",
      "Generating train split: 7112000 examples [00:13, 521309.17 examples/s]\n",
      "Creating CSV from Arrow format: 100%|██████████| 7112/7112 [00:12<00:00, 554.48ba/s]\n",
      "Generating train split: 7371000 examples [00:14, 524667.59 examples/s]\n",
      "Creating CSV from Arrow format: 100%|██████████| 7371/7371 [00:13<00:00, 552.50ba/s]\n",
      "Generating train split: 7289000 examples [00:13, 521496.87 examples/s]\n",
      "Creating CSV from Arrow format: 100%|██████████| 7289/7289 [00:13<00:00, 556.82ba/s]\n",
      "Generating train split: 7278000 examples [00:14, 518981.28 examples/s]\n",
      "Creating CSV from Arrow format: 100%|██████████| 7278/7278 [00:13<00:00, 547.36ba/s]\n",
      "Generating train split: 7350000 examples [00:14, 521765.53 examples/s]\n",
      "Creating CSV from Arrow format: 100%|██████████| 7350/7350 [00:13<00:00, 556.37ba/s]\n",
      "Generating train split: 7243000 examples [00:13, 519351.51 examples/s]\n",
      "Creating CSV from Arrow format: 100%|██████████| 7243/7243 [00:12<00:00, 565.45ba/s]\n",
      "Generating train split: 7261000 examples [00:14, 506998.24 examples/s]\n",
      "Creating CSV from Arrow format: 100%|██████████| 7261/7261 [00:12<00:00, 563.37ba/s]\n",
      "Generating train split: 7185000 examples [00:14, 498154.66 examples/s]\n",
      "Creating CSV from Arrow format: 100%|██████████| 7185/7185 [00:12<00:00, 559.01ba/s]\n",
      "Generating train split: 7296000 examples [00:14, 510316.87 examples/s]\n",
      "Creating CSV from Arrow format: 100%|██████████| 7296/7296 [00:13<00:00, 552.50ba/s]\n",
      "Generating train split: 7971000 examples [00:15, 503912.43 examples/s]\n",
      "Creating CSV from Arrow format: 100%|██████████| 7971/7971 [00:14<00:00, 566.22ba/s]\n",
      "Generating train split: 7059000 examples [00:13, 513025.56 examples/s]\n",
      "Creating CSV from Arrow format: 100%|██████████| 7059/7059 [00:12<00:00, 557.72ba/s]\n",
      "Generating train split: 8142000 examples [00:15, 527271.85 examples/s]\n",
      "Creating CSV from Arrow format: 100%|██████████| 8142/8142 [00:14<00:00, 553.74ba/s]\n",
      "Generating train split: 9622000 examples [00:18, 519633.10 examples/s]\n",
      "Creating CSV from Arrow format: 100%|██████████| 9622/9622 [00:17<00:00, 559.86ba/s]\n",
      "Generating train split: 7678000 examples [00:14, 520845.88 examples/s]\n",
      "Creating CSV from Arrow format: 100%|██████████| 7678/7678 [00:13<00:00, 559.02ba/s]\n",
      "Generating train split: 9173000 examples [00:17, 520727.66 examples/s]\n",
      "Creating CSV from Arrow format: 100%|██████████| 9173/9173 [00:16<00:00, 558.25ba/s]\n",
      "Generating train split: 7695000 examples [00:16, 471405.57 examples/s]\n",
      "Creating CSV from Arrow format: 100%|██████████| 7695/7695 [00:13<00:00, 574.08ba/s]\n",
      "Generating train split: 8540000 examples [00:15, 540569.60 examples/s]\n",
      "Creating CSV from Arrow format: 100%|██████████| 8540/8540 [00:14<00:00, 574.65ba/s]\n",
      "Generating train split: 7206000 examples [00:13, 534183.24 examples/s]\n",
      "Creating CSV from Arrow format: 100%|██████████| 7206/7206 [00:12<00:00, 570.21ba/s]\n",
      "Generating train split: 7717000 examples [00:14, 532481.54 examples/s]\n",
      "Creating CSV from Arrow format: 100%|██████████| 7717/7717 [00:13<00:00, 577.30ba/s]\n",
      "Generating train split: 7405000 examples [00:14, 497311.93 examples/s]\n",
      "Creating CSV from Arrow format: 100%|██████████| 7405/7405 [00:13<00:00, 564.43ba/s]\n",
      "Generating train split: 7075000 examples [00:13, 510032.55 examples/s]\n",
      "Creating CSV from Arrow format: 100%|██████████| 7075/7075 [00:12<00:00, 559.79ba/s]\n",
      "Generating train split: 7249000 examples [00:13, 546823.58 examples/s]\n",
      "Creating CSV from Arrow format: 100%|██████████| 7249/7249 [00:12<00:00, 569.88ba/s]\n",
      "Generating train split: 7652000 examples [00:14, 545310.18 examples/s]\n",
      "Creating CSV from Arrow format: 100%|██████████| 7652/7652 [00:13<00:00, 558.82ba/s]\n",
      "Generating train split: 7203000 examples [00:13, 535004.72 examples/s]\n",
      "Creating CSV from Arrow format: 100%|██████████| 7203/7203 [00:12<00:00, 573.28ba/s]\n",
      "Generating train split: 7955000 examples [00:14, 540127.20 examples/s]\n",
      "Creating CSV from Arrow format: 100%|██████████| 7955/7955 [00:14<00:00, 564.79ba/s]\n"
     ]
    }
   ],
   "source": [
    "Pipeline() \\\n",
    "    .data(read_csv('./data/ecg_preprocessed/*.csv', ['ECG_Clean', 'category'])) \\\n",
    "    .rename({'ECG_Clean': 'signal', 'category': 'label'}) \\\n",
    "    .to(write_csv('./data/ecg_model_with_features/[0-9]{5}.csv'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
