{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from glob import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = './data/ecg_features_60s_clean_twa_rqa_60s'\n",
    "participants = [Path(file).name for file in glob(f'{root}/*.csv')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa66972833894a0c89f8e01ea8579e1e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading dataset shards:   0%|          | 0/17 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = datasets.load_dataset(\n",
    "    root, \n",
    "    train_participants=participants,\n",
    "    trust_remote_code=True,\n",
    "    num_proc=8\n",
    ")['fit']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(baseline = 0, mental_stress = 1, high_physical_activity = 2, moderate_physical_activity = 3, low_physical_activity = 4):\n",
    "    def inner(labels):\n",
    "        baseline_list = ['Sitting', 'Recov1', 'Recov2', 'Recov3', 'Recov4', 'Recov5', 'Recov6']\n",
    "        mental_stress_list = ['TA', 'SSST_Sing_countdown', 'Pasat', 'Raven', 'TA_repeat', 'Pasat_repeat']\n",
    "        high_physical_stress_list = ['Treadmill1', 'Treadmill2', 'Treadmill3', 'Treadmill4', 'Walking_fast_pace', 'Cycling', 'stairs_up_and_down']\n",
    "        moderate_physical_stress_list = ['Walking_own_pace', 'Dishes', 'Vacuum']\n",
    "        low_physical_stress_list = ['Standing', 'Lying_supine', 'Recov_standing']\n",
    "        \n",
    "        def encode_multiclass(label):\n",
    "            if label in baseline_list:\n",
    "                return baseline\n",
    "            elif label in mental_stress_list:\n",
    "                return mental_stress\n",
    "            elif label in high_physical_stress_list:\n",
    "                return high_physical_activity\n",
    "            elif label in moderate_physical_stress_list:\n",
    "                return moderate_physical_activity\n",
    "            elif label in low_physical_stress_list:\n",
    "                return low_physical_activity\n",
    "            else:\n",
    "                return -1\n",
    "            \n",
    "        return {\n",
    "            'label': [encode_multiclass(label) for label in labels],\n",
    "        }\n",
    "    return inner\n",
    "\n",
    "def clean(dataset, mapping={}):\n",
    "    dataset = dataset.map(\n",
    "        encode(**mapping), \n",
    "        batched=True, \n",
    "        batch_size=2048, \n",
    "        input_columns=['label'],\n",
    "        num_proc=4\n",
    "    )\n",
    "    return dataset.filter(\n",
    "        lambda label: label != -1,\n",
    "        input_columns=['label'],\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = clean(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['label', 'hrv_mean', 'hrv_min', 'hrv_max', 'hrv_std', 'hrv_rms', 'hr_mean', 'hr_min', 'hr_max', 'hr_std', 'rr_mean', 'rr_min', 'rr_max', 'rr_std', 'nn50', 'pnn50', 'rmssd', 'ulf_min', 'vlf_min', 'lf_min', 'hf_min', 'vhf_min', 'uhf_min', 'tp_min', 'lf_hf_ratio_min', 'lp_ulf_min', 'lp_vlf_min', 'lp_lf_min', 'lp_hf_min', 'lp_vhf_min', 'lp_uhf_min', 'lf_normalized_min', 'hf_normalized_min', 'lf/hf+lf_min', 'hf/hf+lf_min', 'ulf_max', 'vlf_max', 'lf_max', 'hf_max', 'vhf_max', 'uhf_max', 'tp_max', 'lf_hf_ratio_max', 'lp_ulf_max', 'lp_vlf_max', 'lp_lf_max', 'lp_hf_max', 'lp_vhf_max', 'lp_uhf_max', 'lf_normalized_max', 'hf_normalized_max', 'lf/hf+lf_max', 'hf/hf+lf_max', 'ulf_mean', 'vlf_mean', 'lf_mean', 'hf_mean', 'vhf_mean', 'uhf_mean', 'tp_mean', 'lf_hf_ratio_mean', 'lp_ulf_mean', 'lp_vlf_mean', 'lp_lf_mean', 'lp_hf_mean', 'lp_vhf_mean', 'lp_uhf_mean', 'lf_normalized_mean', 'hf_normalized_mean', 'lf/hf+lf_mean', 'hf/hf+lf_mean', 'ulf_median', 'vlf_median', 'lf_median', 'hf_median', 'vhf_median', 'uhf_median', 'tp_median', 'lf_hf_ratio_median', 'lp_ulf_median', 'lp_vlf_median', 'lp_lf_median', 'lp_hf_median', 'lp_vhf_median', 'lp_uhf_median', 'lf_normalized_median', 'hf_normalized_median', 'lf/hf+lf_median', 'hf/hf+lf_median', 'ulf_std', 'vlf_std', 'lf_std', 'hf_std', 'vhf_std', 'uhf_std', 'tp_std', 'lf_hf_ratio_std', 'lp_ulf_std', 'lp_vlf_std', 'lp_lf_std', 'lp_hf_std', 'lp_vhf_std', 'lp_uhf_std', 'lf_normalized_std', 'hf_normalized_std', 'lf/hf+lf_std', 'hf/hf+lf_std', 'ulf_power', 'vlf_power', 'lf_power', 'hf_power', 'vhf_power', 'uhf_power', 'tp_power', 'lf_hf_ratio_power', 'lp_ulf_power', 'lp_vlf_power', 'lp_lf_power', 'lp_hf_power', 'lp_vhf_power', 'lp_uhf_power', 'lf_normalized_power', 'hf_normalized_power', 'lf/hf+lf_power', 'hf/hf+lf_power', 'ulf_covariance', 'vlf_covariance', 'lf_covariance', 'hf_covariance', 'vhf_covariance', 'uhf_covariance', 'tp_covariance', 'lf_hf_ratio_covariance', 'lp_ulf_covariance', 'lp_vlf_covariance', 'lp_lf_covariance', 'lp_hf_covariance', 'lp_vhf_covariance', 'lp_uhf_covariance', 'lf_normalized_covariance', 'hf_normalized_covariance', 'lf/hf+lf_covariance', 'hf/hf+lf_covariance', 'ulf_energy', 'vlf_energy', 'lf_energy', 'hf_energy', 'vhf_energy', 'uhf_energy', 'tp_energy', 'lf_hf_ratio_energy', 'lp_ulf_energy', 'lp_vlf_energy', 'lp_lf_energy', 'lp_hf_energy', 'lp_vhf_energy', 'lp_uhf_energy', 'lf_normalized_energy', 'hf_normalized_energy', 'lf/hf+lf_energy', 'hf/hf+lf_energy', 'ulf_entropy', 'vlf_entropy', 'lf_entropy', 'hf_entropy', 'vhf_entropy', 'uhf_entropy', 'tp_entropy', 'lf_hf_ratio_entropy', 'lp_ulf_entropy', 'lp_vlf_entropy', 'lp_lf_entropy', 'lp_hf_entropy', 'lp_vhf_entropy', 'lp_uhf_entropy', 'lf_normalized_entropy', 'hf_normalized_entropy', 'lf/hf+lf_entropy', 'hf/hf+lf_entropy', 'w', 'wmax', 'wen', 'MeanNN', 'SDNN', 'SDANN1', 'SDNNI1', 'SDANN2', 'SDNNI2', 'SDANN5', 'SDNNI5', 'RMSSD', 'SDSD', 'CVNN', 'CVSD', 'MedianNN', 'MadNN', 'MCVNN', 'IQRNN', 'SDRMSSD', 'Prc20NN', 'Prc80NN', 'pNN50', 'pNN20', 'MinNN', 'MaxNN', 'HTI', 'TINN', 'SD1', 'SD2', 'SD1SD2', 'S', 'CSI', 'CVI', 'CSI_Modified', 'GI', 'SI', 'AI', 'PI', 'SD1d', 'SD1a', 'C1d', 'C1a', 'SD2d', 'SD2a', 'C2d', 'C2a', 'SDNNd', 'SDNNa', 'Cd', 'Ca', 'PIP', 'IALS', 'PSS', 'PAS', 'ApEn', 'SampEn', 'ShanEn', 'FuzzyEn', 'MSEn', 'CMSEn', 'CD', 'HFD', 'KFD', 'LZC', 'DFA_alpha1', 'DFA_alpha2', 'twa', 'twa_width'],\n",
       "    num_rows: 3636860\n",
       "})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.select_columns(['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = dataset.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "1        1187478\n",
       "0        1040651\n",
       "2         892154\n",
       "4         287103\n",
       "3         229474\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3636860"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.value_counts().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "1        32.7%\n",
       "0        28.6%\n",
       "2        24.5%\n",
       "4         7.9%\n",
       "3         6.3%\n",
       "Name: proportion, dtype: object"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.value_counts(normalize=True).mul(100).round(1).astype(str) + '%'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
