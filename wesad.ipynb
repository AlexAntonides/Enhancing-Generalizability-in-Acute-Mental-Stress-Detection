{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "participants = [Path(path).stem for path in glob('./data/wesad/**') if Path(path).is_dir()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "for participant in participants:\n",
    "    data = pd.read_pickle(f'./data/wesad/{participant}/{participant}.pkl')\n",
    "    df = pd.DataFrame({\n",
    "        'signal': data['signal']['chest']['ECG'].flatten(),\n",
    "        'label': data['label']\n",
    "    })\n",
    "    df.to_csv(f'./data/wesad/{participant}/{participant}.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "from scipy import stats\n",
    "import collections\n",
    "from datasets import load_dataset\n",
    "import neurokit2 as nk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TINN(x:np.array):\n",
    "  \"\"\" Compute all the triangular interpolation to calculate the TINN scores. It also computes HRV index from an array x which contains \n",
    "      all the interbeats times for a given ECG signal.\n",
    "\n",
    "      The axis is divided in 2 parts respectively on the right and left of the abscissa of the maximum value of the gaussian distribution\n",
    "      The TINN score calculation is defined in the WESAD Dataset paper, to calculate it we needthe closest triangular interpolation \n",
    "      of the gaussian distribution of the interbeats times. The triangular interpolation is defined by 2 lines that meet at the maximum value\n",
    "      of the gaussian distribution and cross the x-axis in N on the first half of the x-axis and M on the second half of the x-axis. \n",
    "      Thus inside ]N;M[ the interpolation function != 0\n",
    "      Outside of ]N;M[ the interpolation function equals 0.\n",
    "  \"\"\"\n",
    "\n",
    "  kernel = stats.gaussian_kde(x) #Create an approximated kernel for gaussian distribution from the x array (interbeats times)\n",
    "  absi=np.linspace(np.min(x),np.max(x),len(x)) # Compute the x-axis of the interbeats distribution (from minimum interbeat time to maximum interbeat time)\n",
    "  val=kernel.evaluate(absi) # Fit the gaussian distribution to the created x-axis\n",
    "  ecart=absi[1]-absi[0] # Space between 2 values on the axis\n",
    "  maxind=np.argmax(val) # Select the index for which the gaussian distribution (val array) is maximum \n",
    "  max_pos=absi[maxind]  # Interbeat time (abscissa) for which the gaussian distribution is maximum\n",
    "  maxvalue=np.amax(val) # Max of the gaussian distribution\n",
    "  N_abs=absi[0:maxind+1] # First half of the x-axis\n",
    "  M_abs=absi[maxind:] # Second half of the x-axis\n",
    "  HRVindex=len(x)/maxvalue\n",
    "  err_N=[]\n",
    "  err_M=[]\n",
    "\n",
    "  for i in range(0,len(N_abs)-1):\n",
    "    N=N_abs[i]\n",
    "    slope=(maxvalue)/(max_pos-N)\n",
    "    D=val[0:maxind+1]\n",
    "    q=np.clip(slope*ecart*np.arange(-i,-i+maxind+1),0,None) #Triangular interpolation on the First half of the x-axis\n",
    "    diff=D-q \n",
    "    err=np.multiply(diff,diff)\n",
    "    err1=np.delete(err,-1)\n",
    "    err2=np.delete(err, 0)\n",
    "    errint=(err1+err2)/2\n",
    "    errtot=np.linalg.norm(errint) # Error area between the triangular interpolation and the gaussian distribution on the first half of the x-axis\n",
    "    err_N.append((errtot,N,N_abs,q))\n",
    "  \n",
    "  for i in range(1,len(M_abs)):\n",
    "    M=M_abs[i]\n",
    "    slope=(maxvalue)/(max_pos-M)\n",
    "    D=val[maxind:]\n",
    "    q=np.clip(slope*ecart*np.arange(-i,len(D)-i),0,None) #Triangular interpolation on the second half of the x-axis\n",
    "    diff=D-q\n",
    "    err=np.multiply(diff,diff)\n",
    "    err1=np.delete(err,-1)\n",
    "    err2=np.delete(err, 0)\n",
    "    errint=(err1+err2)/2\n",
    "    errtot=np.linalg.norm(errint) # Error area between the triangular interpolation and the gaussian distribution on the second half of the x-axis\n",
    "    err_M.append((errtot,M,M_abs,q))\n",
    "\n",
    "  return (err_N,err_M,absi,val,HRVindex)\n",
    "\n",
    "def best_TINN(x:np.array):\n",
    "  \"\"\"Select the best N and M that give the best triangular interpolation function approximation of the gaussian distrbution and return\n",
    "    N; M; the TINN score = M-N ; and the HRV index\n",
    "  \n",
    "  \"\"\"\n",
    "  err_N,err_M,_,_,HRVindex=TINN(x)\n",
    "  N=np.argmin(np.array(err_N,dtype=object)[:,0])\n",
    "  M=np.argmin(np.array(err_M,dtype=object)[:,0])\n",
    "  absN=err_N[N][1]\n",
    "  absM=err_M[M][1]\n",
    "  return float(absN),float(absM),float(absM-absN),HRVindex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_freq_features_ecg(x):\n",
    "  \"\"\" Returns frequential features of the Heart Rate Variability signal (interbeats times) by computing FFT, to compute the Fouriers \n",
    "  Frequencies the mean of the Heart Rate variability is used as sampling period  \n",
    "  \"\"\"\n",
    "  mean=np.mean(x)\n",
    "  yf=np.array(scipy.fft.fft(x-mean))\n",
    "  xf=scipy.fft.fftfreq(len(x),mean)[0:len(x)//2]\n",
    "  psd=(2/len(yf))*np.abs(yf)[0:len(x)//2]\n",
    "  fmean=np.mean(xf)\n",
    "  fstd=np.std(xf)\n",
    "  sumpsd=np.sum(psd)\n",
    "  return fmean,fstd,sumpsd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_compare_NN50(x,i):\n",
    "  \"\"\"Count the number of HRV intervals differing more than 50 ms for a given HRV interval x[i]\n",
    "  \n",
    "  \"\"\"\n",
    "  ref=x[i]\n",
    "  k=0\n",
    "  diff=np.absolute(x-ref)\n",
    "  k+=np.sum(np.where(diff>0.05,1,0))\n",
    "  return k \n",
    "\n",
    "def compare_NN50(x):\n",
    "  \"\"\" Returns the number and percentage of HRV intervals differing more than 50ms for all intervals\n",
    "  \n",
    "  \"\"\"\n",
    "  k=0\n",
    "  for i in range(0,len(x)):\n",
    "    k+=num_compare_NN50(x,i)\n",
    "  if k==0:\n",
    "    k=1\n",
    "  return k,(k/(len(x)*len(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampling_rate = 700\n",
    "n_window = 20 * sampling_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_shift_size = 1\n",
    "step_size = int(window_shift_size * sampling_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_and_save_participant(participant):\n",
    "    dataset = load_dataset('csv', data_files=f'./data/wesad/{participant}/{participant}.csv')['train']\n",
    "\n",
    "    # 1 = baseline (neutral).\n",
    "    neutral = dataset.filter(lambda x: x['label'] == 1).select(range(90 * sampling_rate))\n",
    "    \n",
    "    neutral_peaks, _ = nk.ecg_peaks(neutral['signal'], sampling_rate=sampling_rate)\n",
    "    neutral_peaks_indices = neutral_peaks[neutral_peaks['ECG_R_Peaks'] == 1].index\n",
    "\n",
    "    ## HR\n",
    "    neutral_signal_rate = nk.signal_rate(neutral_peaks, sampling_rate=sampling_rate)\n",
    "    neutral_mean_hr = np.mean(neutral_signal_rate)\n",
    "    neutral_std_hr = np.std(neutral_signal_rate)\n",
    "\n",
    "    ## Frequencies\n",
    "    neutral_periods = np.array([(neutral_peaks_indices[i+1]-neutral_peaks_indices[i])/sampling_rate for i in range(0,len(neutral_peaks_indices)-1)])\n",
    "    neutral_frequency = 1 / neutral_periods\n",
    "    neutral_mean_freq = np.mean(neutral_frequency)\n",
    "    neutral_std_freq = np.std(neutral_frequency)\n",
    "    neutral_mean_f, neutral_std_f, neutral_sum_psd = get_freq_features_ecg(neutral_periods)\n",
    "    \n",
    "    ## HRV\n",
    "    neutral_hrv = np.array([(neutral_peaks_indices[i]-neutral_peaks_indices[i-1])/sampling_rate for i in range(1,len(neutral_peaks_indices))])\n",
    "    neutral_mean_hrv = np.mean(neutral_hrv)\n",
    "    neutral_std_hrv = np.std(neutral_hrv)\n",
    "    neutral_rms_hrv = np.sqrt(np.mean(neutral_hrv**2))\n",
    "    _, _, _, neutral_hrv_index = best_TINN(neutral_hrv)\n",
    "\n",
    "    ## %NN50\n",
    "    neutral_NN50, neutral_pNN50 = compare_NN50(neutral_hrv)\n",
    "\n",
    "    ## Power\n",
    "    neutral_frequencies = nk.hrv_frequency(\n",
    "        neutral_peaks, \n",
    "        sampling_rate=sampling_rate,\n",
    "        ulf=[0.01,0.04],\n",
    "        lf=[0.04,0.15],\n",
    "        hf=[0.15,0.4],\n",
    "        vhf=[0.4,1]\n",
    "    )\n",
    "    neutral_total_power = np.nansum([neutral_frequencies['HRV_ULF'], neutral_frequencies['HRV_LF'], neutral_frequencies['HRV_HF'], neutral_frequencies['HRV_VHF']])\n",
    "\n",
    "    dataframes = []\n",
    "    for start_idx in range(0, len(dataset), step_size): ## Window shift\n",
    "        try:\n",
    "            sample = dataset[start_idx:start_idx+n_window]\n",
    "            if len(sample['signal']) < n_window:\n",
    "                continue\n",
    "            \n",
    "            signal = sample['signal']\n",
    "            label = collections.Counter(sample['label']).most_common(1)[0][0]\n",
    "\n",
    "            peaks, _ = nk.ecg_peaks(signal, sampling_rate=sampling_rate)\n",
    "            peaks_indices = peaks[peaks['ECG_R_Peaks'] == 1].index\n",
    "\n",
    "            ## HR\n",
    "            signal_rate = nk.signal_rate(peaks, sampling_rate=sampling_rate)\n",
    "            mean_hr = np.mean(signal_rate)\n",
    "            std_hr = np.std(signal_rate)\n",
    "\n",
    "            ## Frequencies\n",
    "            periods = np.array([(peaks_indices[i+1]-peaks_indices[i])/sampling_rate for i in range(0,len(peaks_indices)-1)])\n",
    "            frequency = 1 / periods\n",
    "            mean_freq = np.mean(frequency)\n",
    "            std_freq = np.std(frequency)\n",
    "            mean_f, std_f, sum_psd = get_freq_features_ecg(periods)\n",
    "            \n",
    "            ## HRV\n",
    "            hrv = np.array([(peaks_indices[i]-peaks_indices[i-1])/sampling_rate for i in range(1,len(peaks_indices))])\n",
    "            mean_hrv = np.mean(hrv)\n",
    "            std_hrv = np.std(hrv)\n",
    "            rms_hrv = np.sqrt(np.mean(hrv**2))\n",
    "            _, _, _, hrv_index = best_TINN(hrv)\n",
    "\n",
    "            ## %NN50\n",
    "            NN50, pNN50 = compare_NN50(hrv)\n",
    "\n",
    "            ## Power\n",
    "            frequencies = nk.hrv_frequency(\n",
    "                peaks, \n",
    "                sampling_rate=sampling_rate,\n",
    "                ulf=[0.01,0.04],\n",
    "                lf=[0.04,0.15],\n",
    "                hf=[0.15,0.4],\n",
    "                vhf=[0.4,1]\n",
    "            )\n",
    "            total_power = np.nansum([frequencies['HRV_ULF'], frequencies['HRV_LF'], frequencies['HRV_HF'], frequencies['HRV_VHF']])\n",
    "\n",
    "            ## Dataframe\n",
    "            df = nk.hrv(peaks, sampling_rate=sampling_rate)\n",
    "            df['label'] = label\n",
    "            df['mean_hr'] = mean_hr / neutral_mean_hr\n",
    "            df['std_hr'] = std_hr / neutral_std_hr\n",
    "            df['hrv_index'] = hrv_index / neutral_hrv_index\n",
    "            df['nn50'] = NN50 / neutral_NN50\n",
    "            df['mean_hrv'] = mean_hrv / neutral_mean_hrv\n",
    "            df['std_hrv'] = std_hrv / neutral_std_hrv\n",
    "            df['rms_hrv'] = rms_hrv / neutral_rms_hrv\n",
    "            df['mean_fourier_frequencies'] = mean_f / neutral_mean_f\n",
    "            df['std_fourier_frequencies'] = std_f / neutral_std_f\n",
    "            df['sum_psd'] = sum_psd / neutral_sum_psd\n",
    "            df['ulf'] = frequencies['HRV_ULF'] / neutral_frequencies['HRV_ULF']\n",
    "            df['lf'] = frequencies['HRV_LF'] / neutral_frequencies['HRV_LF']\n",
    "            df['hf'] = frequencies['HRV_HF'] / neutral_frequencies['HRV_HF']\n",
    "            df['uhf'] = frequencies['HRV_VHF'] / neutral_frequencies['HRV_VHF']\n",
    "            df['lf_hf_ratio'] = (frequencies['HRV_LF'] / frequencies['HRV_HF']) / (neutral_frequencies['HRV_LF'] / neutral_frequencies['HRV_HF'])\n",
    "            df['total_power'] = total_power / neutral_total_power\n",
    "            df['relative_power_ulf'] = ((frequencies['HRV_ULF'] / total_power) * 100) / ((neutral_frequencies['HRV_ULF'] / neutral_total_power) * 100)\n",
    "            df['relative_power_lf'] = ((frequencies['HRV_LF'] / total_power) * 100) / ((neutral_frequencies['HRV_LF'] / neutral_total_power) * 100)\n",
    "            df['relative_power_hf'] = ((frequencies['HRV_HF'] / total_power) * 100) / ((neutral_frequencies['HRV_HF'] / neutral_total_power) * 100)\n",
    "            df['relative_power_uhf'] = ((frequencies['HRV_VHF'] / total_power) * 100) / ((neutral_frequencies['HRV_VHF'] / neutral_total_power) * 100)\n",
    "            dataframes.append(df)\n",
    "        except Exception as e:\n",
    "            # print(e)\n",
    "            continue\n",
    "        \n",
    "    result = pd.concat(dataframes, ignore_index=True)\n",
    "    result.to_csv(f'./data/wesad_features_20s/{participant}.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import Parallel, delayed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Parallel(n_jobs=6)(delayed(process_and_save_participant)(participant) for participant in participants) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
