{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "from scipy import stats\n",
    "import collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import neurokit2 as nk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "from scipy import stats\n",
    "\n",
    "def TINN(x:np.array):\n",
    "  \"\"\" Compute all the triangular interpolation to calculate the TINN scores. It also computes HRV index from an array x which contains \n",
    "      all the interbeats times for a given ECG signal.\n",
    "\n",
    "      The axis is divided in 2 parts respectively on the right and left of the abscissa of the maximum value of the gaussian distribution\n",
    "      The TINN score calculation is defined in the WESAD Dataset paper, to calculate it we needthe closest triangular interpolation \n",
    "      of the gaussian distribution of the interbeats times. The triangular interpolation is defined by 2 lines that meet at the maximum value\n",
    "      of the gaussian distribution and cross the x-axis in N on the first half of the x-axis and M on the second half of the x-axis. \n",
    "      Thus inside ]N;M[ the interpolation function != 0\n",
    "      Outside of ]N;M[ the interpolation function equals 0.\n",
    "  \"\"\"\n",
    "\n",
    "  kernel = stats.gaussian_kde(x) #Create an approximated kernel for gaussian distribution from the x array (interbeats times)\n",
    "  absi=np.linspace(np.min(x),np.max(x),len(x)) # Compute the x-axis of the interbeats distribution (from minimum interbeat time to maximum interbeat time)\n",
    "  val=kernel.evaluate(absi) # Fit the gaussian distribution to the created x-axis\n",
    "  ecart=absi[1]-absi[0] # Space between 2 values on the axis\n",
    "  maxind=np.argmax(val) # Select the index for which the gaussian distribution (val array) is maximum \n",
    "  max_pos=absi[maxind]  # Interbeat time (abscissa) for which the gaussian distribution is maximum\n",
    "  maxvalue=np.amax(val) # Max of the gaussian distribution\n",
    "  N_abs=absi[0:maxind+1] # First half of the x-axis\n",
    "  M_abs=absi[maxind:] # Second half of the x-axis\n",
    "  HRVindex=len(x)/maxvalue\n",
    "  err_N=[]\n",
    "  err_M=[]\n",
    "\n",
    "  for i in range(0,len(N_abs)-1):\n",
    "    N=N_abs[i]\n",
    "    slope=(maxvalue)/(max_pos-N)\n",
    "    D=val[0:maxind+1]\n",
    "    q=np.clip(slope*ecart*np.arange(-i,-i+maxind+1),0,None) #Triangular interpolation on the First half of the x-axis\n",
    "    diff=D-q \n",
    "    err=np.multiply(diff,diff)\n",
    "    err1=np.delete(err,-1)\n",
    "    err2=np.delete(err, 0)\n",
    "    errint=(err1+err2)/2\n",
    "    errtot=np.linalg.norm(errint) # Error area between the triangular interpolation and the gaussian distribution on the first half of the x-axis\n",
    "    err_N.append((errtot,N,N_abs,q))\n",
    "  \n",
    "  for i in range(1,len(M_abs)):\n",
    "    M=M_abs[i]\n",
    "    slope=(maxvalue)/(max_pos-M)\n",
    "    D=val[maxind:]\n",
    "    q=np.clip(slope*ecart*np.arange(-i,len(D)-i),0,None) #Triangular interpolation on the second half of the x-axis\n",
    "    diff=D-q\n",
    "    err=np.multiply(diff,diff)\n",
    "    err1=np.delete(err,-1)\n",
    "    err2=np.delete(err, 0)\n",
    "    errint=(err1+err2)/2\n",
    "    errtot=np.linalg.norm(errint) # Error area between the triangular interpolation and the gaussian distribution on the second half of the x-axis\n",
    "    err_M.append((errtot,M,M_abs,q))\n",
    "\n",
    "  return (err_N,err_M,absi,val,HRVindex)\n",
    "\n",
    "def best_TINN(x:np.array):\n",
    "  \"\"\"Select the best N and M that give the best triangular interpolation function approximation of the gaussian distrbution and return\n",
    "    N; M; the TINN score = M-N ; and the HRV index\n",
    "  \n",
    "  \"\"\"\n",
    "  err_N,err_M,_,_,HRVindex=TINN(x)\n",
    "  N=np.argmin(np.array(err_N,dtype=object)[:,0])\n",
    "  M=np.argmin(np.array(err_M,dtype=object)[:,0])\n",
    "  absN=err_N[N][1]\n",
    "  absM=err_M[M][1]\n",
    "  return float(absN),float(absM),float(absM-absN),HRVindex\n",
    "\n",
    "# _,_,T,HRVindex=best_TINN(hrv)\n",
    "# T, HRVindex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_compare_NN50(x,i):\n",
    "  \"\"\"Count the number of HRV intervals differing more than 50 ms for a given HRV interval x[i]\n",
    "  \n",
    "  \"\"\"\n",
    "  ref=x[i]\n",
    "  k=0\n",
    "  diff=np.absolute(x-ref)\n",
    "  k+=np.sum(np.where(diff>0.05,1,0))\n",
    "  return k \n",
    "\n",
    "def compare_NN50(x):\n",
    "  \"\"\" Returns the number and percentage of HRV intervals differing more than 50ms for all intervals\n",
    "  \n",
    "  \"\"\"\n",
    "  k=0\n",
    "  for i in range(0,len(x)):\n",
    "    k+=num_compare_NN50(x,i)\n",
    "  if k==0:\n",
    "    k=1\n",
    "  return k,(k/(len(x)*len(x)))\n",
    "\n",
    "# num50,p50=compare_NN50(hrv)\n",
    "# num50, p50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_freq_features_ecg(x):\n",
    "  \"\"\" Returns frequential features of the Heart Rate Variability signal (interbeats times) by computing FFT, to compute the Fouriers \n",
    "  Frequencies the mean of the Heart Rate variability is used as sampling period  \n",
    "  \"\"\"\n",
    "  mean=np.mean(x)\n",
    "  yf=np.array(scipy.fft.fft(x-mean))\n",
    "  xf=scipy.fft.fftfreq(len(x),mean)[0:len(x)//2]\n",
    "  psd=(2/len(yf))*np.abs(yf)[0:len(x)//2]\n",
    "  fmean=np.mean(xf)\n",
    "  fstd=np.std(xf)\n",
    "  sumpsd=np.sum(psd)\n",
    "  return fmean,fstd,sumpsd\n",
    "\n",
    "# fmean,fstd,sumpsd=get_freq_features_ecg(hrv)\n",
    "# fmean,fstd, sumpsd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/Edouard99/Stress_Detection_ECG/tree/main\n",
    "# pd.DataFrame({\n",
    "#     'meanHR': meanHR,\n",
    "#     'stdHR': stdHR,\n",
    "#     'TINN': hrv_indices['HRV_TINN'],\n",
    "#     'HRVindex': HRVindex,\n",
    "#     '%NN50': num50,\n",
    "#     'pnn50': hrv_indices['HRV_pNN50'],\n",
    "#     'meanHRV': meanHRV,\n",
    "#     'stdHRV': stdHRV,\n",
    "#     'rmsHRV': rmsHRV,\n",
    "#     'Mean Fourier Frequencies': fmean,\n",
    "#     'STD Fourier Frequencies': fstd,\n",
    "#     'Sum PSD components': sumpsd\n",
    "# }, index=[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://dl-acm-org.vu-nl.idm.oclc.org/doi/epdf/10.1145/3242969.3242985\n",
    "# pd.DataFrame({\n",
    "#     'μHR': meanHR,\n",
    "#     'σHR': stdHR,\n",
    "#     'μHRV': meanHRV,\n",
    "#     'σHRV': stdHRV,\n",
    "#     'NN50': num50, \n",
    "#     'pNN50': hrv_indices['HRV_pNN50'],\n",
    "#     'TINN': hrv_indices['HRV_TINN'],\n",
    "#     'rmsHRV': rmsHRV,\n",
    "#     'ULF': frequencies['HRV_ULF'],\n",
    "#     'LF': frequencies['HRV_LF'],\n",
    "#     'HF': frequencies['HRV_HF'],\n",
    "#     'UHF': frequencies['HRV_VHF'],\n",
    "#     'LF_HF_Ratio': frequencies['HRV_LF'] / frequencies['HRV_HF'],\n",
    "#     'total_power': total_power,\n",
    "#     'relative_power_ulf': (frequencies['HRV_ULF'] / total_power) * 100,\n",
    "#     'relative_power_lf': (frequencies['HRV_LF'] / total_power) * 100,\n",
    "#     'relative_power_hf': (frequencies['HRV_HF'] / total_power) * 100,\n",
    "#     'relative_power_vhf': (frequencies['HRV_VHF'] / total_power) * 100,\n",
    "#     'LF_norm': np.nan,  ## Can only be normalised after all the LF and HF are calculated\n",
    "#     'HF_norm': np.nan,  ## Can only be normalised after all the LF and HF are calculated\n",
    "# }, index=[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The “heart rate” can be described as a true rate in beats per minute (HR) or as the RR interval in milliseconds. \n",
    "The RR interval is the time elapsed between two successive R waves of the QRS signal on the electrocardiogram\n",
    "“Heart rate variability” has become the conventionally accepted term to describe variations of both instantaneous heart rate and RR intervals.\n",
    "\n",
    "The RR interval and HR are hyperbolically related (HR x RR interval = 60000; see figure 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_average_t(epochs, sampling_rate=1000):\n",
    "    try:\n",
    "        average = nk.epochs_average(epochs, which=\"Signal\")\n",
    "\n",
    "        min_length = sampling_rate * 4\n",
    "        n = len(average)\n",
    "        # Unfortunately, NeuroKit is unable to detect the P-QRST-T peaks on a single heartbeat. It requires multiple, so let's just repeat the signal to satisfy the minimum length required.\n",
    "        signal = np.concatenate( [list(average['Signal_Mean']) for _ in range(math.ceil(min_length / n))])\n",
    "\n",
    "        _, waves_grand = nk.ecg_delineate(signal, rpeaks=None, method='peak', sampling_rate=sampling_rate)\n",
    "        t_index = waves_grand['ECG_T_Peaks'][0]\n",
    "\n",
    "        if np.isnan(t_index):\n",
    "            return np.nan\n",
    "        else:\n",
    "            return signal[t_index]\n",
    "    except:\n",
    "        return np.nan\n",
    "\n",
    "def modified_moving_average(signal, sampling_rate=1000):\n",
    "    epochs = nk.ecg_segment(signal, rpeaks=None, sampling_rate=sampling_rate)\n",
    "\n",
    "    if len(epochs) % 2 != 0:\n",
    "        # We want balanced buckets, so pop the last item to make it even.\n",
    "        epochs.popitem()\n",
    "    \n",
    "    even_keys = list(epochs.keys())[1::2]\n",
    "    odd_keys = list(epochs.keys())[::2]\n",
    "\n",
    "    even_bucket = {key: epochs[key] for key in even_keys}\n",
    "    odd_bucket = {key: epochs[key] for key in odd_keys}\n",
    "\n",
    "    average_t_even = _get_average_t(even_bucket, sampling_rate)\n",
    "    average_t_odd = _get_average_t(odd_bucket, sampling_rate)\n",
    "\n",
    "    if average_t_even is None or average_t_odd is None:\n",
    "        return np.nan\n",
    "    else:\n",
    "        twa = abs(average_t_even - average_t_odd)\n",
    "        return twa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from glob import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_window = 60 * 1000\n",
    "sampling_rate = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_shift_size = 0.25\n",
    "step_size = int(window_shift_size * sampling_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = glob('../data/ecg_preprocessed/*.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline = ['Sitting', 'Recov1', 'Recov2', 'Recov3', 'Recov4', 'Recov5', 'Recov6']\n",
    "mental_stress = ['TA', 'SSST_Sing_countdown', 'Pasat', 'Raven', 'TA_repeat', 'Pasat_repeat']\n",
    "high_physical_stress = ['Treadmill1', 'Treadmill2', 'Treadmill3', 'Treadmill4', 'Walking_fast_pace', 'Cycling', 'stairs_up_and_down']\n",
    "moderate_physical_stress = ['Walking_own_pace', 'Dishes', 'Vacuum']\n",
    "low_physical_stress = ['Standing', 'Lying_supine', 'Recov_standing']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.signal import find_peaks\n",
    "from scipy.fft import fft, fftfreq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyhrv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_fr_n(freq, max_freq, l ):\n",
    "    if freq < max_freq:\n",
    "        return int(freq * l/max_freq)\n",
    "    else:\n",
    "        return l - 1\n",
    "    \n",
    "def detect_peaks_ECG(peaks, window_size,timestep_data,distance):\n",
    "    # f_p = find_peaks(sample, distance=distance)# height = 0.4, distance = distance)\n",
    "    #time features\n",
    "    # f_p_diff = np.diff(f_p[0]) * timestep_data\n",
    "    f_p_diff = np.diff(peaks) * timestep_data\n",
    "    \n",
    "    # heart rate mean std min max \n",
    "    HR_mean = (60/f_p_diff).mean()\n",
    "    HR_std = (60/f_p_diff).std()\n",
    "    HR_max = (60/f_p_diff).max()\n",
    "    HR_min = (60/f_p_diff).min()\n",
    "    #NN50\n",
    "    #pNN50\n",
    "    NN50 = sum(np.abs(np.diff(f_p_diff)) > 0.050)\n",
    "    N_HRV_50 = NN50\n",
    "    P_HRV_50 = NN50/len(f_p_diff)\n",
    "    #rr_features\n",
    "    rmssd = np.sqrt(np.mean(np.square(np.diff(f_p_diff))))\n",
    "    rr_mean = f_p_diff.mean()\n",
    "    rr_std = f_p_diff.std()\n",
    "    # freq features\n",
    "    # f_p_diff_fft = savgol_filter(np.diff(f_p_diff), 5,2)\n",
    "    \n",
    "    T = window_size * timestep_data\n",
    "    k = np.arange(len(f_p_diff))\n",
    "    freqs = k/T\n",
    "    m = freqs.max()/2\n",
    "    l = int(len(freqs)/2)\n",
    "    ffts = abs(np.fft.fft(f_p_diff)*np.hamming(len(k)))**2\n",
    "    ULF = sum( ffts[ f_fr_n(0.01,m,l):f_fr_n(0.04,m,l) ] )\n",
    "    HF = sum( ffts[ f_fr_n(0.15,m,l):f_fr_n(0.4,m,l) ] )\n",
    "    LF = sum( ffts[ f_fr_n(0.04,m,l):f_fr_n(0.15,m,l) ] )\n",
    "    UHF = sum( ffts[ f_fr_n(0.4,m,l):f_fr_n(1,m,l) ] )\n",
    "    \n",
    "    TP = ULF + LF + HF + UHF\n",
    "\n",
    "    rate_L_H = LF/HF\n",
    "    lfN = LF / TP \n",
    "    hfN = HF / TP\n",
    "    \n",
    "    return {\n",
    "        'μhr' : HR_mean,\n",
    "        'σhr' : HR_std,\n",
    "        'HR_max': HR_max,\n",
    "        'HR_min' : HR_min,\n",
    "        'NN50' : N_HRV_50,\n",
    "        'pNN50' : P_HRV_50,\n",
    "        'rmssd' : rmssd,\n",
    "        'rr_mean' : rr_mean,\n",
    "        'rr_std' : rr_std,\n",
    "        'ULF' : ULF,\n",
    "        'HF': HF,\n",
    "        'LF': LF,\n",
    "        'UHF': UHF,\n",
    "        'LF_HF_ratio': rate_L_H,\n",
    "        'Σ': TP,\n",
    "        'relative_power_ULF': (ULF / TP) * 100,\n",
    "        'relative_power_LF': (LF / TP) * 100,\n",
    "        'relative_power_HF': (HF / TP) * 100,\n",
    "        'relative_power_UHF': (UHF / TP) * 100,\n",
    "        'LF_norm': lfN,\n",
    "        'HF_norm': hfN,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(idxs):\n",
    "\n",
    "\n",
    "def preprocess_and_save(file):\n",
    "    dataset = load_dataset(\n",
    "        '../data/ecg_preprocessed', \n",
    "        data_files=[f'{Path(file).stem}.csv'],\n",
    "    )['train']\n",
    "    \n",
    "    data = []\n",
    "    # with tqdm(total=len(dataset)) as pbar:\n",
    "    for start_idx in range(0, len(dataset), step_size): ## Window shift\n",
    "        try:\n",
    "            if start_idx + n_window > len(dataset):\n",
    "                break\n",
    "            sample = dataset[start_idx:start_idx+n_window]\n",
    "            if len(sample['ECG_Clean']) < n_window:\n",
    "                continue\n",
    "\n",
    "            label = collections.Counter(sample['category']).most_common(1)[0][0]\n",
    "\n",
    "            peaks, _ = nk.ecg_peaks(sample['ECG_Clean'], sampling_rate=sampling_rate)\n",
    "            peaks_indices = peaks[peaks['ECG_R_Peaks'] == 1].index\n",
    "            \n",
    "            ## HRV\n",
    "            hrv = np.array([(peaks_indices[i]-peaks_indices[i-1])/sampling_rate for i in range(1,len(peaks_indices))])\n",
    "            mean_hrv = np.mean(hrv)\n",
    "            std_hrv = np.std(hrv)\n",
    "            rms_hrv = np.sqrt(np.mean(hrv**2))\n",
    "            _,_,tinn,_ = best_TINN(hrv) \n",
    "\n",
    "            r_peaks = nk.ecg_findpeaks(sample['ECG_Clean'])['ECG_R_Peaks']\n",
    "            fp_data = detect_peaks_ECG(r_peaks, n_window, 1/sampling_rate, 200)\n",
    "\n",
    "            twa = modified_moving_average(sample['ECG_Clean'], sampling_rate)\n",
    "            \n",
    "            data.append({\n",
    "                'label': label,\n",
    "                'μhr': fp_data['μhr'],\n",
    "                'σhr': fp_data['σhr'],\n",
    "                'μhrv': mean_hrv,\n",
    "                'σhrv': std_hrv,\n",
    "                'NN50': fp_data['NN50'],\n",
    "                'pNN50': fp_data['pNN50'],\n",
    "                'TINN': tinn,\n",
    "                'rmsHRV': rms_hrv,\n",
    "                'ULF': fp_data['ULF'],\n",
    "                'LF': fp_data['LF'],\n",
    "                'HF': fp_data['HF'],\n",
    "                'UHF': fp_data['UHF'],\n",
    "                'LF_HF_ratio': fp_data['LF_HF_ratio'],\n",
    "                'Σ': fp_data['Σ'],\n",
    "                'relative_power_ulf': fp_data['relative_power_ULF'],\n",
    "                'relative_power_lf': fp_data['relative_power_LF'],\n",
    "                'relative_power_hf': fp_data['relative_power_HF'],\n",
    "                'relative_power_uhf': fp_data['relative_power_UHF'],\n",
    "                'LF_norm': fp_data['HF_norm'],\n",
    "                'HF_norm': fp_data['HF_norm'],\n",
    "                'hr_max': fp_data['HR_max'],\n",
    "                'hr_min': fp_data['HR_min'],\n",
    "                'rmssd': fp_data['rmssd'],\n",
    "                'rr_mean': fp_data['rr_mean'],\n",
    "                'rr_std': fp_data['rr_std'],\n",
    "                'twa': twa\n",
    "            })\n",
    "            # pbar.update(step_size)\n",
    "        except Exception as e:\n",
    "            print(file, \":\", start_idx, \"->\", e)\n",
    "            # pbar.update(step_size)\n",
    "            continue\n",
    "        \n",
    "    result = pd.DataFrame(data)\n",
    "\n",
    "    stem = Path(file).stem\n",
    "    result.to_csv(f'../data/ecg_features_60s_clean_twa/{stem}.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import Parallel, delayed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[47], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m files:\n\u001b[1;32m----> 2\u001b[0m     \u001b[43mpreprocess_and_save\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[45], line 13\u001b[0m, in \u001b[0;36mpreprocess_and_save\u001b[1;34m(file)\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m start_idx \u001b[38;5;241m+\u001b[39m n_window \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mlen\u001b[39m(dataset):\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m---> 13\u001b[0m sample \u001b[38;5;241m=\u001b[39m \u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43mstart_idx\u001b[49m\u001b[43m:\u001b[49m\u001b[43mstart_idx\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43mn_window\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(sample[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mECG_Clean\u001b[39m\u001b[38;5;124m'\u001b[39m]) \u001b[38;5;241m<\u001b[39m n_window:\n\u001b[0;32m     15\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\datasets\\arrow_dataset.py:2810\u001b[0m, in \u001b[0;36mDataset.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   2808\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, key):  \u001b[38;5;66;03m# noqa: F811\u001b[39;00m\n\u001b[0;32m   2809\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Can be used to index columns (by string names) or rows (by integer index or iterable of indices or bools).\"\"\"\u001b[39;00m\n\u001b[1;32m-> 2810\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_getitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\datasets\\arrow_dataset.py:2795\u001b[0m, in \u001b[0;36mDataset._getitem\u001b[1;34m(self, key, **kwargs)\u001b[0m\n\u001b[0;32m   2793\u001b[0m formatter \u001b[38;5;241m=\u001b[39m get_formatter(format_type, features\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_info\u001b[38;5;241m.\u001b[39mfeatures, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mformat_kwargs)\n\u001b[0;32m   2794\u001b[0m pa_subtable \u001b[38;5;241m=\u001b[39m query_table(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data, key, indices\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_indices)\n\u001b[1;32m-> 2795\u001b[0m formatted_output \u001b[38;5;241m=\u001b[39m \u001b[43mformat_table\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2796\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpa_subtable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mformatter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mformatter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mformat_columns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mformat_columns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_all_columns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_all_columns\u001b[49m\n\u001b[0;32m   2797\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2798\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m formatted_output\n",
      "File \u001b[1;32mc:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\datasets\\formatting\\formatting.py:629\u001b[0m, in \u001b[0;36mformat_table\u001b[1;34m(table, key, formatter, format_columns, output_all_columns)\u001b[0m\n\u001b[0;32m    627\u001b[0m python_formatter \u001b[38;5;241m=\u001b[39m PythonFormatter(features\u001b[38;5;241m=\u001b[39mformatter\u001b[38;5;241m.\u001b[39mfeatures)\n\u001b[0;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m format_columns \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 629\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mformatter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpa_table\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquery_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquery_type\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    630\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m query_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumn\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    631\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m format_columns:\n",
      "File \u001b[1;32mc:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\datasets\\formatting\\formatting.py:400\u001b[0m, in \u001b[0;36mFormatter.__call__\u001b[1;34m(self, pa_table, query_type)\u001b[0m\n\u001b[0;32m    398\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mformat_column(pa_table)\n\u001b[0;32m    399\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m query_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 400\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpa_table\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\datasets\\formatting\\formatting.py:449\u001b[0m, in \u001b[0;36mPythonFormatter.format_batch\u001b[1;34m(self, pa_table)\u001b[0m\n\u001b[0;32m    447\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m LazyBatch(pa_table, \u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m    448\u001b[0m batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpython_arrow_extractor()\u001b[38;5;241m.\u001b[39mextract_batch(pa_table)\n\u001b[1;32m--> 449\u001b[0m batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpython_features_decoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    450\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m batch\n",
      "File \u001b[1;32mc:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\datasets\\formatting\\formatting.py:220\u001b[0m, in \u001b[0;36mPythonFeaturesDecoder.decode_batch\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    217\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecode_column\u001b[39m(\u001b[38;5;28mself\u001b[39m, column: \u001b[38;5;28mlist\u001b[39m, column_name: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mlist\u001b[39m:\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeatures\u001b[38;5;241m.\u001b[39mdecode_column(column, column_name) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeatures \u001b[38;5;28;01melse\u001b[39;00m column\n\u001b[1;32m--> 220\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecode_batch\u001b[39m(\u001b[38;5;28mself\u001b[39m, batch: \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mdict\u001b[39m:\n\u001b[0;32m    221\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeatures\u001b[38;5;241m.\u001b[39mdecode_batch(batch) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeatures \u001b[38;5;28;01melse\u001b[39;00m batch\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for file in files:\n",
    "    preprocess_and_save(file)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parallel(n_jobs=6)(delayed(preprocess_and_save)(file) for file in files) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
