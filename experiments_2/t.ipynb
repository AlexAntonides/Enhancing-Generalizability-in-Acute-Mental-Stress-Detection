{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import datasets\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "from scipy import stats\n",
    "import collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import neurokit2 as nk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "from scipy import stats\n",
    "\n",
    "def TINN(x:np.array):\n",
    "  \"\"\" Compute all the triangular interpolation to calculate the TINN scores. It also computes HRV index from an array x which contains \n",
    "      all the interbeats times for a given ECG signal.\n",
    "\n",
    "      The axis is divided in 2 parts respectively on the right and left of the abscissa of the maximum value of the gaussian distribution\n",
    "      The TINN score calculation is defined in the WESAD Dataset paper, to calculate it we needthe closest triangular interpolation \n",
    "      of the gaussian distribution of the interbeats times. The triangular interpolation is defined by 2 lines that meet at the maximum value\n",
    "      of the gaussian distribution and cross the x-axis in N on the first half of the x-axis and M on the second half of the x-axis. \n",
    "      Thus inside ]N;M[ the interpolation function != 0\n",
    "      Outside of ]N;M[ the interpolation function equals 0.\n",
    "  \"\"\"\n",
    "\n",
    "  kernel = stats.gaussian_kde(x) #Create an approximated kernel for gaussian distribution from the x array (interbeats times)\n",
    "  absi=np.linspace(np.min(x),np.max(x),len(x)) # Compute the x-axis of the interbeats distribution (from minimum interbeat time to maximum interbeat time)\n",
    "  val=kernel.evaluate(absi) # Fit the gaussian distribution to the created x-axis\n",
    "  ecart=absi[1]-absi[0] # Space between 2 values on the axis\n",
    "  maxind=np.argmax(val) # Select the index for which the gaussian distribution (val array) is maximum \n",
    "  max_pos=absi[maxind]  # Interbeat time (abscissa) for which the gaussian distribution is maximum\n",
    "  maxvalue=np.amax(val) # Max of the gaussian distribution\n",
    "  N_abs=absi[0:maxind+1] # First half of the x-axis\n",
    "  M_abs=absi[maxind:] # Second half of the x-axis\n",
    "  HRVindex=len(x)/maxvalue\n",
    "  err_N=[]\n",
    "  err_M=[]\n",
    "\n",
    "  for i in range(0,len(N_abs)-1):\n",
    "    N=N_abs[i]\n",
    "    slope=(maxvalue)/(max_pos-N)\n",
    "    D=val[0:maxind+1]\n",
    "    q=np.clip(slope*ecart*np.arange(-i,-i+maxind+1),0,None) #Triangular interpolation on the First half of the x-axis\n",
    "    diff=D-q \n",
    "    err=np.multiply(diff,diff)\n",
    "    err1=np.delete(err,-1)\n",
    "    err2=np.delete(err, 0)\n",
    "    errint=(err1+err2)/2\n",
    "    errtot=np.linalg.norm(errint) # Error area between the triangular interpolation and the gaussian distribution on the first half of the x-axis\n",
    "    err_N.append((errtot,N,N_abs,q))\n",
    "  \n",
    "  for i in range(1,len(M_abs)):\n",
    "    M=M_abs[i]\n",
    "    slope=(maxvalue)/(max_pos-M)\n",
    "    D=val[maxind:]\n",
    "    q=np.clip(slope*ecart*np.arange(-i,len(D)-i),0,None) #Triangular interpolation on the second half of the x-axis\n",
    "    diff=D-q\n",
    "    err=np.multiply(diff,diff)\n",
    "    err1=np.delete(err,-1)\n",
    "    err2=np.delete(err, 0)\n",
    "    errint=(err1+err2)/2\n",
    "    errtot=np.linalg.norm(errint) # Error area between the triangular interpolation and the gaussian distribution on the second half of the x-axis\n",
    "    err_M.append((errtot,M,M_abs,q))\n",
    "\n",
    "  return (err_N,err_M,absi,val,HRVindex)\n",
    "\n",
    "def best_TINN(x:np.array):\n",
    "  \"\"\"Select the best N and M that give the best triangular interpolation function approximation of the gaussian distrbution and return\n",
    "    N; M; the TINN score = M-N ; and the HRV index\n",
    "  \n",
    "  \"\"\"\n",
    "  err_N,err_M,_,_,HRVindex=TINN(x)\n",
    "  N=np.argmin(np.array(err_N,dtype=object)[:,0])\n",
    "  M=np.argmin(np.array(err_M,dtype=object)[:,0])\n",
    "  absN=err_N[N][1]\n",
    "  absM=err_M[M][1]\n",
    "  return float(absN),float(absM),float(absM-absN),HRVindex\n",
    "\n",
    "# _,_,T,HRVindex=best_TINN(hrv)\n",
    "# T, HRVindex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_compare_NN50(x,i):\n",
    "  \"\"\"Count the number of HRV intervals differing more than 50 ms for a given HRV interval x[i]\n",
    "  \n",
    "  \"\"\"\n",
    "  ref=x[i]\n",
    "  k=0\n",
    "  diff=np.absolute(x-ref)\n",
    "  k+=np.sum(np.where(diff>0.05,1,0))\n",
    "  return k \n",
    "\n",
    "def compare_NN50(x):\n",
    "  \"\"\" Returns the number and percentage of HRV intervals differing more than 50ms for all intervals\n",
    "  \n",
    "  \"\"\"\n",
    "  k=0\n",
    "  for i in range(0,len(x)):\n",
    "    k+=num_compare_NN50(x,i)\n",
    "  if k==0:\n",
    "    k=1\n",
    "  return k,(k/(len(x)*len(x)))\n",
    "\n",
    "# num50,p50=compare_NN50(hrv)\n",
    "# num50, p50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_freq_features_ecg(x):\n",
    "  \"\"\" Returns frequential features of the Heart Rate Variability signal (interbeats times) by computing FFT, to compute the Fouriers \n",
    "  Frequencies the mean of the Heart Rate variability is used as sampling period  \n",
    "  \"\"\"\n",
    "  mean=np.mean(x)\n",
    "  yf=np.array(scipy.fft.fft(x-mean))\n",
    "  xf=scipy.fft.fftfreq(len(x),mean)[0:len(x)//2]\n",
    "  psd=(2/len(yf))*np.abs(yf)[0:len(x)//2]\n",
    "  fmean=np.mean(xf)\n",
    "  fstd=np.std(xf)\n",
    "  sumpsd=np.sum(psd)\n",
    "  return fmean,fstd,sumpsd\n",
    "\n",
    "# fmean,fstd,sumpsd=get_freq_features_ecg(hrv)\n",
    "# fmean,fstd, sumpsd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/Edouard99/Stress_Detection_ECG/tree/main\n",
    "# pd.DataFrame({\n",
    "#     'meanHR': meanHR,\n",
    "#     'stdHR': stdHR,\n",
    "#     'TINN': hrv_indices['HRV_TINN'],\n",
    "#     'HRVindex': HRVindex,\n",
    "#     '%NN50': num50,\n",
    "#     'pnn50': hrv_indices['HRV_pNN50'],\n",
    "#     'meanHRV': meanHRV,\n",
    "#     'stdHRV': stdHRV,\n",
    "#     'rmsHRV': rmsHRV,\n",
    "#     'Mean Fourier Frequencies': fmean,\n",
    "#     'STD Fourier Frequencies': fstd,\n",
    "#     'Sum PSD components': sumpsd\n",
    "# }, index=[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://dl-acm-org.vu-nl.idm.oclc.org/doi/epdf/10.1145/3242969.3242985\n",
    "# pd.DataFrame({\n",
    "#     'μHR': meanHR,\n",
    "#     'σHR': stdHR,\n",
    "#     'μHRV': meanHRV,\n",
    "#     'σHRV': stdHRV,\n",
    "#     'NN50': num50, \n",
    "#     'pNN50': hrv_indices['HRV_pNN50'],\n",
    "#     'TINN': hrv_indices['HRV_TINN'],\n",
    "#     'rmsHRV': rmsHRV,\n",
    "#     'ULF': frequencies['HRV_ULF'],\n",
    "#     'LF': frequencies['HRV_LF'],\n",
    "#     'HF': frequencies['HRV_HF'],\n",
    "#     'UHF': frequencies['HRV_VHF'],\n",
    "#     'LF_HF_Ratio': frequencies['HRV_LF'] / frequencies['HRV_HF'],\n",
    "#     'total_power': total_power,\n",
    "#     'relative_power_ulf': (frequencies['HRV_ULF'] / total_power) * 100,\n",
    "#     'relative_power_lf': (frequencies['HRV_LF'] / total_power) * 100,\n",
    "#     'relative_power_hf': (frequencies['HRV_HF'] / total_power) * 100,\n",
    "#     'relative_power_vhf': (frequencies['HRV_VHF'] / total_power) * 100,\n",
    "#     'LF_norm': np.nan,  ## Can only be normalised after all the LF and HF are calculated\n",
    "#     'HF_norm': np.nan,  ## Can only be normalised after all the LF and HF are calculated\n",
    "# }, index=[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The “heart rate” can be described as a true rate in beats per minute (HR) or as the RR interval in milliseconds. \n",
    "The RR interval is the time elapsed between two successive R waves of the QRS signal on the electrocardiogram\n",
    "“Heart rate variability” has become the conventionally accepted term to describe variations of both instantaneous heart rate and RR intervals.\n",
    "\n",
    "The RR interval and HR are hyperbolically related (HR x RR interval = 60000; see figure 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_average_t(epochs, sampling_rate=1000):\n",
    "    try:\n",
    "        average = nk.epochs_average(epochs, which=\"Signal\")\n",
    "\n",
    "        min_length = sampling_rate * 4\n",
    "        n = len(average)\n",
    "        # Unfortunately, NeuroKit is unable to detect the P-QRST-T peaks on a single heartbeat. It requires multiple, so let's just repeat the signal to satisfy the minimum length required.\n",
    "        signal = np.concatenate( [list(average['Signal_Mean']) for _ in range(math.ceil(min_length / n))])\n",
    "\n",
    "        _, waves_grand = nk.ecg_delineate(signal, rpeaks=None, method='peak', sampling_rate=sampling_rate)\n",
    "        t_index = waves_grand['ECG_T_Peaks'][0]\n",
    "\n",
    "        if np.isnan(t_index):\n",
    "            return np.nan\n",
    "        else:\n",
    "            return signal[t_index]\n",
    "    except:\n",
    "        return np.nan\n",
    "\n",
    "def modified_moving_average(signal, sampling_rate=1000):\n",
    "    epochs = nk.ecg_segment(signal, rpeaks=None, sampling_rate=sampling_rate)\n",
    "\n",
    "    if len(epochs) % 2 != 0:\n",
    "        # We want balanced buckets, so pop the last item to make it even.\n",
    "        epochs.popitem()\n",
    "    \n",
    "    even_keys = list(epochs.keys())[1::2]\n",
    "    odd_keys = list(epochs.keys())[::2]\n",
    "\n",
    "    even_bucket = {key: epochs[key] for key in even_keys}\n",
    "    odd_bucket = {key: epochs[key] for key in odd_keys}\n",
    "\n",
    "    average_t_even = _get_average_t(even_bucket, sampling_rate)\n",
    "    average_t_odd = _get_average_t(odd_bucket, sampling_rate)\n",
    "\n",
    "    if average_t_even is None or average_t_odd is None:\n",
    "        return np.nan\n",
    "    else:\n",
    "        twa = abs(average_t_even - average_t_odd)\n",
    "        return twa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from glob import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_window = 60 * 1000\n",
    "sampling_rate = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_shift_size = 0.25\n",
    "step_size = int(window_shift_size * sampling_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = glob('../data/ecg_preprocessed/*.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline = ['Sitting', 'Recov1', 'Recov2', 'Recov3', 'Recov4', 'Recov5', 'Recov6']\n",
    "mental_stress = ['TA', 'SSST_Sing_countdown', 'Pasat', 'Raven', 'TA_repeat', 'Pasat_repeat']\n",
    "high_physical_stress = ['Treadmill1', 'Treadmill2', 'Treadmill3', 'Treadmill4', 'Walking_fast_pace', 'Cycling', 'stairs_up_and_down']\n",
    "moderate_physical_stress = ['Walking_own_pace', 'Dishes', 'Vacuum']\n",
    "low_physical_stress = ['Standing', 'Lying_supine', 'Recov_standing']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.signal import find_peaks\n",
    "from scipy.fft import fft, fftfreq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyhrv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_fr_n(freq, max_freq, l ):\n",
    "    if freq < max_freq:\n",
    "        return int(freq * l/max_freq)\n",
    "    else:\n",
    "        return l - 1\n",
    "    \n",
    "def detect_peaks_ECG(peaks, window_size,timestep_data,distance):\n",
    "    # f_p = find_peaks(sample, distance=distance)# height = 0.4, distance = distance)\n",
    "    #time features\n",
    "    # f_p_diff = np.diff(f_p[0]) * timestep_data\n",
    "    f_p_diff = np.diff(peaks) * timestep_data\n",
    "    \n",
    "    # heart rate mean std min max \n",
    "    HR_mean = (60/f_p_diff).mean()\n",
    "    HR_std = (60/f_p_diff).std()\n",
    "    HR_max = (60/f_p_diff).max()\n",
    "    HR_min = (60/f_p_diff).min()\n",
    "    #NN50\n",
    "    #pNN50\n",
    "    NN50 = sum(np.abs(np.diff(f_p_diff)) > 0.050)\n",
    "    N_HRV_50 = NN50\n",
    "    P_HRV_50 = NN50/len(f_p_diff)\n",
    "    #rr_features\n",
    "    rmssd = np.sqrt(np.mean(np.square(np.diff(f_p_diff))))\n",
    "    rr_mean = f_p_diff.mean()\n",
    "    rr_std = f_p_diff.std()\n",
    "    # freq features\n",
    "    # f_p_diff_fft = savgol_filter(np.diff(f_p_diff), 5,2)\n",
    "    \n",
    "    T = window_size * timestep_data\n",
    "    k = np.arange(len(f_p_diff))\n",
    "    freqs = k/T\n",
    "    m = freqs.max()/2\n",
    "    l = int(len(freqs)/2)\n",
    "    ffts = abs(np.fft.fft(f_p_diff)*np.hamming(len(k)))**2\n",
    "    ULF = sum( ffts[ f_fr_n(0.01,m,l):f_fr_n(0.04,m,l) ] )\n",
    "    HF = sum( ffts[ f_fr_n(0.15,m,l):f_fr_n(0.4,m,l) ] )\n",
    "    LF = sum( ffts[ f_fr_n(0.04,m,l):f_fr_n(0.15,m,l) ] )\n",
    "    UHF = sum( ffts[ f_fr_n(0.4,m,l):f_fr_n(1,m,l) ] )\n",
    "    \n",
    "    TP = ULF + LF + HF + UHF\n",
    "\n",
    "    rate_L_H = LF/HF\n",
    "    lfN = LF / TP \n",
    "    hfN = HF / TP\n",
    "    \n",
    "    return {\n",
    "        'μhr' : HR_mean,\n",
    "        'σhr' : HR_std,\n",
    "        'HR_max': HR_max,\n",
    "        'HR_min' : HR_min,\n",
    "        'NN50' : N_HRV_50,\n",
    "        'pNN50' : P_HRV_50,\n",
    "        'rmssd' : rmssd,\n",
    "        'rr_mean' : rr_mean,\n",
    "        'rr_std' : rr_std,\n",
    "        'ULF' : ULF,\n",
    "        'HF': HF,\n",
    "        'LF': LF,\n",
    "        'UHF': UHF,\n",
    "        'LF_HF_ratio': rate_L_H,\n",
    "        'Σ': TP,\n",
    "        'relative_power_ULF': (ULF / TP) * 100,\n",
    "        'relative_power_LF': (LF / TP) * 100,\n",
    "        'relative_power_HF': (HF / TP) * 100,\n",
    "        'relative_power_UHF': (UHF / TP) * 100,\n",
    "        'LF_norm': lfN,\n",
    "        'HF_norm': hfN,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_and_save(file):\n",
    "    dataset = load_dataset(\n",
    "        '../data/ecg_preprocessed', \n",
    "        data_files=[f'{Path(file).stem}.csv'],\n",
    "    )['train']\n",
    "    \n",
    "    data = []\n",
    "    # with tqdm(total=len(dataset)) as pbar:\n",
    "    for start_idx in range(0, len(dataset), step_size): ## Window shift\n",
    "        try:\n",
    "            if start_idx + n_window > len(dataset):\n",
    "                break\n",
    "            sample = dataset[start_idx:start_idx+n_window]\n",
    "            if len(sample['ECG_Clean']) < n_window:\n",
    "                continue\n",
    "\n",
    "            label = collections.Counter(sample['category']).most_common(1)[0][0]\n",
    "\n",
    "            peaks, _ = nk.ecg_peaks(sample['ECG_Clean'], sampling_rate=sampling_rate)\n",
    "            peaks_indices = peaks[peaks['ECG_R_Peaks'] == 1].index\n",
    "            \n",
    "            ## HRV\n",
    "            hrv = np.array([(peaks_indices[i]-peaks_indices[i-1])/sampling_rate for i in range(1,len(peaks_indices))])\n",
    "            mean_hrv = np.mean(hrv)\n",
    "            std_hrv = np.std(hrv)\n",
    "            rms_hrv = np.sqrt(np.mean(hrv**2))\n",
    "            _,_,tinn,_ = best_TINN(hrv) \n",
    "\n",
    "            r_peaks = nk.ecg_findpeaks(sample['ECG_Clean'])['ECG_R_Peaks']\n",
    "            fp_data = detect_peaks_ECG(r_peaks, n_window, 1/sampling_rate, 200)\n",
    "\n",
    "            twa = modified_moving_average(sample['ECG_Clean'], sampling_rate)\n",
    "            \n",
    "            data.append({\n",
    "                'label': label,\n",
    "                'μhr': fp_data['μhr'],\n",
    "                'σhr': fp_data['σhr'],\n",
    "                'μhrv': mean_hrv,\n",
    "                'σhrv': std_hrv,\n",
    "                'NN50': fp_data['NN50'],\n",
    "                'pNN50': fp_data['pNN50'],\n",
    "                'TINN': tinn,\n",
    "                'rmsHRV': rms_hrv,\n",
    "                'ULF': fp_data['ULF'],\n",
    "                'LF': fp_data['LF'],\n",
    "                'HF': fp_data['HF'],\n",
    "                'UHF': fp_data['UHF'],\n",
    "                'LF_HF_ratio': fp_data['LF_HF_ratio'],\n",
    "                'Σ': fp_data['Σ'],\n",
    "                'relative_power_ulf': fp_data['relative_power_ULF'],\n",
    "                'relative_power_lf': fp_data['relative_power_LF'],\n",
    "                'relative_power_hf': fp_data['relative_power_HF'],\n",
    "                'relative_power_uhf': fp_data['relative_power_UHF'],\n",
    "                'LF_norm': fp_data['HF_norm'],\n",
    "                'HF_norm': fp_data['HF_norm'],\n",
    "                'hr_max': fp_data['HR_max'],\n",
    "                'hr_min': fp_data['HR_min'],\n",
    "                'rmssd': fp_data['rmssd'],\n",
    "                'rr_mean': fp_data['rr_mean'],\n",
    "                'rr_std': fp_data['rr_std'],\n",
    "                'twa': twa\n",
    "            })\n",
    "            # pbar.update(step_size)\n",
    "        except Exception as e:\n",
    "            print(file, \":\", start_idx, \"->\", e)\n",
    "            # pbar.update(step_size)\n",
    "            continue\n",
    "        \n",
    "    result = pd.DataFrame(data)\n",
    "\n",
    "    stem = Path(file).stem\n",
    "    result.to_csv(f'../data/ecg_features_60s_clean_twa/{stem}.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import Parallel, delayed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for file in files:\n",
    "#     preprocess_and_save(file)\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[24], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mParallel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m6\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpreprocess_and_save\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mfile\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mfiles\u001b[49m\u001b[43m)\u001b[49m \n",
      "File \u001b[1;32mc:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\joblib\\parallel.py:1952\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1946\u001b[0m \u001b[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[0;32m   1947\u001b[0m \u001b[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[0;32m   1948\u001b[0m \u001b[38;5;66;03m# reach the first `yield` statement. This starts the aynchronous\u001b[39;00m\n\u001b[0;32m   1949\u001b[0m \u001b[38;5;66;03m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[0;32m   1950\u001b[0m \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[1;32m-> 1952\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\joblib\\parallel.py:1595\u001b[0m, in \u001b[0;36mParallel._get_outputs\u001b[1;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[0;32m   1592\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[0;32m   1594\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mretrieval_context():\n\u001b[1;32m-> 1595\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retrieve()\n\u001b[0;32m   1597\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mGeneratorExit\u001b[39;00m:\n\u001b[0;32m   1598\u001b[0m     \u001b[38;5;66;03m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[0;32m   1599\u001b[0m     \u001b[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[0;32m   1600\u001b[0m     \u001b[38;5;66;03m# the user if necessary.\u001b[39;00m\n\u001b[0;32m   1601\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\joblib\\parallel.py:1707\u001b[0m, in \u001b[0;36mParallel._retrieve\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1702\u001b[0m \u001b[38;5;66;03m# If the next job is not ready for retrieval yet, we just wait for\u001b[39;00m\n\u001b[0;32m   1703\u001b[0m \u001b[38;5;66;03m# async callbacks to progress.\u001b[39;00m\n\u001b[0;32m   1704\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ((\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[0;32m   1705\u001b[0m     (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mget_status(\n\u001b[0;32m   1706\u001b[0m         timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeout) \u001b[38;5;241m==\u001b[39m TASK_PENDING)):\n\u001b[1;32m-> 1707\u001b[0m     time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m0.01\u001b[39m)\n\u001b[0;32m   1708\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m   1710\u001b[0m \u001b[38;5;66;03m# We need to be careful: the job list can be filling up as\u001b[39;00m\n\u001b[0;32m   1711\u001b[0m \u001b[38;5;66;03m# we empty it and Python list are not thread-safe by\u001b[39;00m\n\u001b[0;32m   1712\u001b[0m \u001b[38;5;66;03m# default hence the use of the lock\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "Parallel(n_jobs=6)(delayed(preprocess_and_save)(file) for file in files) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
