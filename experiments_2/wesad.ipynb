{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "participants = [Path(path).stem for path in glob('./data/wesad/**') if Path(path).is_dir()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "for participant in participants:\n",
    "    data = pd.read_pickle(f'./data/wesad/{participant}/{participant}.pkl')\n",
    "    df = pd.DataFrame({\n",
    "        'signal': data['signal']['chest']['ECG'].flatten(),\n",
    "        'label': data['label']\n",
    "    })\n",
    "    df.to_csv(f'./data/wesad/{participant}/{participant}.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "from scipy import stats\n",
    "import collections\n",
    "from datasets import load_dataset\n",
    "import neurokit2 as nk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TINN(x:np.array):\n",
    "  \"\"\" Compute all the triangular interpolation to calculate the TINN scores. It also computes HRV index from an array x which contains \n",
    "      all the interbeats times for a given ECG signal.\n",
    "\n",
    "      The axis is divided in 2 parts respectively on the right and left of the abscissa of the maximum value of the gaussian distribution\n",
    "      The TINN score calculation is defined in the WESAD Dataset paper, to calculate it we needthe closest triangular interpolation \n",
    "      of the gaussian distribution of the interbeats times. The triangular interpolation is defined by 2 lines that meet at the maximum value\n",
    "      of the gaussian distribution and cross the x-axis in N on the first half of the x-axis and M on the second half of the x-axis. \n",
    "      Thus inside ]N;M[ the interpolation function != 0\n",
    "      Outside of ]N;M[ the interpolation function equals 0.\n",
    "  \"\"\"\n",
    "\n",
    "  kernel = stats.gaussian_kde(x) #Create an approximated kernel for gaussian distribution from the x array (interbeats times)\n",
    "  absi=np.linspace(np.min(x),np.max(x),len(x)) # Compute the x-axis of the interbeats distribution (from minimum interbeat time to maximum interbeat time)\n",
    "  val=kernel.evaluate(absi) # Fit the gaussian distribution to the created x-axis\n",
    "  ecart=absi[1]-absi[0] # Space between 2 values on the axis\n",
    "  maxind=np.argmax(val) # Select the index for which the gaussian distribution (val array) is maximum \n",
    "  max_pos=absi[maxind]  # Interbeat time (abscissa) for which the gaussian distribution is maximum\n",
    "  maxvalue=np.amax(val) # Max of the gaussian distribution\n",
    "  N_abs=absi[0:maxind+1] # First half of the x-axis\n",
    "  M_abs=absi[maxind:] # Second half of the x-axis\n",
    "  HRVindex=len(x)/maxvalue\n",
    "  err_N=[]\n",
    "  err_M=[]\n",
    "\n",
    "  for i in range(0,len(N_abs)-1):\n",
    "    N=N_abs[i]\n",
    "    slope=(maxvalue)/(max_pos-N)\n",
    "    D=val[0:maxind+1]\n",
    "    q=np.clip(slope*ecart*np.arange(-i,-i+maxind+1),0,None) #Triangular interpolation on the First half of the x-axis\n",
    "    diff=D-q \n",
    "    err=np.multiply(diff,diff)\n",
    "    err1=np.delete(err,-1)\n",
    "    err2=np.delete(err, 0)\n",
    "    errint=(err1+err2)/2\n",
    "    errtot=np.linalg.norm(errint) # Error area between the triangular interpolation and the gaussian distribution on the first half of the x-axis\n",
    "    err_N.append((errtot,N,N_abs,q))\n",
    "  \n",
    "  for i in range(1,len(M_abs)):\n",
    "    M=M_abs[i]\n",
    "    slope=(maxvalue)/(max_pos-M)\n",
    "    D=val[maxind:]\n",
    "    q=np.clip(slope*ecart*np.arange(-i,len(D)-i),0,None) #Triangular interpolation on the second half of the x-axis\n",
    "    diff=D-q\n",
    "    err=np.multiply(diff,diff)\n",
    "    err1=np.delete(err,-1)\n",
    "    err2=np.delete(err, 0)\n",
    "    errint=(err1+err2)/2\n",
    "    errtot=np.linalg.norm(errint) # Error area between the triangular interpolation and the gaussian distribution on the second half of the x-axis\n",
    "    err_M.append((errtot,M,M_abs,q))\n",
    "\n",
    "  return (err_N,err_M,absi,val,HRVindex)\n",
    "\n",
    "def best_TINN(x:np.array):\n",
    "  \"\"\"Select the best N and M that give the best triangular interpolation function approximation of the gaussian distrbution and return\n",
    "    N; M; the TINN score = M-N ; and the HRV index\n",
    "  \n",
    "  \"\"\"\n",
    "  err_N,err_M,_,_,HRVindex=TINN(x)\n",
    "  N=np.argmin(np.array(err_N,dtype=object)[:,0])\n",
    "  M=np.argmin(np.array(err_M,dtype=object)[:,0])\n",
    "  absN=err_N[N][1]\n",
    "  absM=err_M[M][1]\n",
    "  return float(absN),float(absM),float(absM-absN),HRVindex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_freq_features_ecg(x):\n",
    "  \"\"\" Returns frequential features of the Heart Rate Variability signal (interbeats times) by computing FFT, to compute the Fouriers \n",
    "  Frequencies the mean of the Heart Rate variability is used as sampling period  \n",
    "  \"\"\"\n",
    "  mean=np.mean(x)\n",
    "  yf=np.array(scipy.fft.fft(x-mean))\n",
    "  xf=scipy.fft.fftfreq(len(x),mean)[0:len(x)//2]\n",
    "  psd=(2/len(yf))*np.abs(yf)[0:len(x)//2]\n",
    "  fmean=np.mean(xf)\n",
    "  fstd=np.std(xf)\n",
    "  sumpsd=np.sum(psd)\n",
    "  return fmean,fstd,sumpsd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_compare_NN50(x,i):\n",
    "  \"\"\"Count the number of HRV intervals differing more than 50 ms for a given HRV interval x[i]\n",
    "  \n",
    "  \"\"\"\n",
    "  ref=x[i]\n",
    "  k=0\n",
    "  diff=np.absolute(x-ref)\n",
    "  k+=np.sum(np.where(diff>0.05,1,0))\n",
    "  return k \n",
    "\n",
    "def compare_NN50(x):\n",
    "  \"\"\" Returns the number and percentage of HRV intervals differing more than 50ms for all intervals\n",
    "  \n",
    "  \"\"\"\n",
    "  k=0\n",
    "  for i in range(0,len(x)):\n",
    "    k+=num_compare_NN50(x,i)\n",
    "  if k==0:\n",
    "    k=1\n",
    "  return k,(k/(len(x)*len(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampling_rate = 700\n",
    "n_window = 60 * sampling_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_shift_size = 0.25\n",
    "step_size = int(window_shift_size * sampling_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.signal import find_peaks\n",
    "from scipy.fft import fft, fftfreq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def f_fr_n(freq, max_freq, l ):\n",
    "    if freq < max_freq:\n",
    "        return int(freq * l/max_freq)\n",
    "    else:\n",
    "        return l - 1\n",
    "    \n",
    "def detect_peaks_ECG(sample, window_size,timestep_data,distance):\n",
    "    f_p = find_peaks(sample, height = 0.4, distance = distance)\n",
    "    #time features\n",
    "    f_p_diff = np.diff(f_p[0]) * timestep_data\n",
    "    \n",
    "    # heart rate mean std min max \n",
    "    HR_mean = (60/f_p_diff).mean()\n",
    "    HR_std = (60/f_p_diff).std()\n",
    "    HR_max = (60/f_p_diff).max()\n",
    "    HR_min = (60/f_p_diff).max()\n",
    "    #NN50\n",
    "    #pNN50\n",
    "    NN50 = sum(np.abs(np.diff(f_p_diff)) > 0.050)\n",
    "    N_HRV_50 = NN50\n",
    "    P_HRV_50 = NN50/len(f_p_diff)\n",
    "    #rr_features\n",
    "    rmssd = np.sqrt(np.mean(np.square(np.diff(f_p_diff))))\n",
    "    rr_mean = f_p_diff.mean()\n",
    "    rr_std = f_p_diff.std()\n",
    "    # freq features\n",
    "    # f_p_diff_fft = savgol_filter(np.diff(f_p_diff), 5,2)\n",
    "    \n",
    "    T = window_size * timestep_data\n",
    "    k = np.arange(len(f_p_diff))\n",
    "    freqs = k/T\n",
    "    m = freqs.max()/2\n",
    "    l = int(len(freqs)/2)\n",
    "    ffts = abs(np.fft.fft(f_p_diff)*np.hamming(len(k)))**2\n",
    "    ULF = sum( ffts[ f_fr_n(0.01,m,l):f_fr_n(0.04,m,l) ] )\n",
    "    HF = sum( ffts[ f_fr_n(0.15,m,l):f_fr_n(0.4,m,l) ] )\n",
    "    LF = sum( ffts[ f_fr_n(0.04,m,l):f_fr_n(0.15,m,l) ] )\n",
    "    UHF = sum( ffts[ f_fr_n(0.4,m,l):f_fr_n(1,m,l) ] )\n",
    "    \n",
    "    TP = ULF + LF + HF + UHF\n",
    "\n",
    "    rate_L_H = LF/HF\n",
    "    lfN = LF / TP \n",
    "    hfN = HF / TP\n",
    "    \n",
    "    return {\n",
    "        'μhr' : HR_mean,\n",
    "        'σhr' : HR_std,\n",
    "        # 'HR_max': HR_max,\n",
    "        # 'HR_min' : HR_min,\n",
    "        'NN50' : N_HRV_50,\n",
    "        'pNN50' : P_HRV_50,\n",
    "        # 'rmssd' : rmssd,\n",
    "        # 'rr_mean' : rr_mean,\n",
    "        # 'rr_std' : rr_std,\n",
    "        'ULF' : ULF,\n",
    "        'HF': HF,\n",
    "        'LF': LF,\n",
    "        'UHF': UHF,\n",
    "        'LF_HF_ratio': rate_L_H,\n",
    "        'Σ': TP,\n",
    "        'relative_power_ULF': (ULF / TP) * 100,\n",
    "        'relative_power_LF': (LF / TP) * 100,\n",
    "        'relative_power_HF': (HF / TP) * 100,\n",
    "        'relative_power_UHF': (UHF / TP) * 100,\n",
    "        'LF_norm': lfN,\n",
    "        'HF_norm': hfN,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_and_save_participant(participant):\n",
    "    dataset = load_dataset('csv', data_files=f'./data/wesad/{participant}/{participant}.csv')['train']\n",
    "\n",
    "    dataframes = []\n",
    "    with tqdm(total=len(dataset)) as pbar:\n",
    "        for start_idx in range(0, len(dataset), step_size): ## Window shift\n",
    "            sample = dataset[start_idx:start_idx+n_window]\n",
    "            if len(sample['signal']) < n_window:\n",
    "                continue\n",
    "            \n",
    "            signal = sample['signal']\n",
    "            label = collections.Counter(sample['label']).most_common(1)[0][0]\n",
    "\n",
    "            peaks, _ = nk.ecg_peaks(signal, sampling_rate=sampling_rate)\n",
    "            peaks_indices = peaks[peaks['ECG_R_Peaks'] == 1].index\n",
    "\n",
    "            ## HR\n",
    "            # signal_rate = nk.signal_rate(peaks, sampling_rate=sampling_rate)\n",
    "            # mean_hr = np.mean(signal_rate)\n",
    "            # std_hr = np.std(signal_rate)\n",
    "\n",
    "            ## Periods\n",
    "            # print(\"Getting Periods...\")\n",
    "            # periods = np.array([(peaks_indices[i+1]-peaks_indices[i])/sampling_rate for i in range(0,len(peaks_indices)-1)])\n",
    "            # frequency = 1 / periods\n",
    "            # mean_freq = np.mean(frequency)\n",
    "            # std_freq = np.std(frequency)\n",
    "            # mean_f, std_f, sum_psd = get_freq_features_ecg(periods)\n",
    "            \n",
    "            ## HRV\n",
    "            hrv = np.array([(peaks_indices[i]-peaks_indices[i-1])/sampling_rate for i in range(1,len(peaks_indices))])\n",
    "            mean_hrv = np.mean(hrv)\n",
    "            std_hrv = np.std(hrv)\n",
    "            rms_hrv = np.sqrt(np.mean(hrv**2))\n",
    "            # _, _, _, hrv_index = best_TINN(hrv)\n",
    "\n",
    "            ## %NN50\n",
    "            # NN50, pNN50 = compare_NN50(hrv)\n",
    "            \n",
    "            # rr_intervals = np.diff(find_peaks(sample['signal'])[0])\n",
    "            # rr_fft = fft(rr_intervals)  \n",
    "            # n_samples = len(rr_intervals)\n",
    "            # rr_frequencies = fftfreq(n_samples, d=1/sampling_rate) \n",
    "            # # ULF band isolation\n",
    "            # ulf_band_mask = (rr_frequencies >= 0.01) & (rr_frequencies <= 0.04) \n",
    "            # lf_band_mask = (rr_frequencies >= 0.04) & (rr_frequencies <= 0.15) \n",
    "            # hf_band_mask = (rr_frequencies >= 0.15) & (rr_frequencies <= 0.4) \n",
    "            # uhf_band_mask = (rr_frequencies >= 0.4) & (rr_frequencies <= 1) \n",
    "            # ulf_fft = rr_fft[ulf_band_mask]\n",
    "            # lf_fft = rr_fft[lf_band_mask]\n",
    "            # hf_fft = rr_fft[hf_band_mask]\n",
    "            # uhf_fft = rr_fft[uhf_band_mask]\n",
    "\n",
    "            # power calculation\n",
    "            # ulf = np.sum(np.abs(ulf_fft)**2)\n",
    "            # lf = np.sum(np.abs(lf_fft)**2)\n",
    "            # hf = np.sum(np.abs(hf_fft)**2)\n",
    "            # uhf = np.sum(np.abs(uhf_fft)**2)\n",
    "\n",
    "            # ulf = nk.signal_power(sample['signal'], [0.01, 0.04], sampling_rate=sampling_rate)['Hz_0.01_0.04']\n",
    "            # lf = nk.signal_power(sample['signal'], [0.04, 0.15], sampling_rate=sampling_rate)['Hz_0.04_0.15']\n",
    "            # hf = nk.signal_power(sample['signal'], [0.15, 0.4], sampling_rate=sampling_rate)['Hz_0.15_0.4']\n",
    "            # uhf = nk.signal_power(sample['signal'], [0.4, 1], sampling_rate=sampling_rate)['Hz_0.4_1']\n",
    "            # tp = ulf + lf + hf + uhf\n",
    "            # lfhf = lf/hf\n",
    "            # lfN = lf / tp \n",
    "            # hfN = hf / tp\n",
    "\n",
    "            ## Dataframe\n",
    "            hrv = nk.hrv(peaks, sampling_rate=sampling_rate)\n",
    "\n",
    "            fp_data = detect_peaks_ECG(sample['signal'], n_window, 1/sampling_rate, 200)\n",
    "\n",
    "            df = pd.DataFrame({\n",
    "                'label': label,\n",
    "                'μhr': fp_data['μhr'],\n",
    "                'σhr': fp_data['σhr'],\n",
    "                'μhrv': mean_hrv,\n",
    "                'σhrv': std_hrv,\n",
    "                'NN50': fp_data['NN50'],\n",
    "                'pNN50': fp_data['pNN50'],\n",
    "                'TINN': hrv['HRV_TINN'],\n",
    "                'rmsHRV': rms_hrv,\n",
    "                'ULF': fp_data['ULF'],\n",
    "                'LF': fp_data['LF'],\n",
    "                'HF': fp_data['HF'],\n",
    "                'UHF': fp_data['UHF'],\n",
    "                'LF_HF_ratio': fp_data['LF_HF_ratio'],\n",
    "                'Σ': fp_data['Σ'],\n",
    "                'relative_power_ulf': fp_data['relative_power_ULF'],\n",
    "                'relative_power_lf': fp_data['relative_power_LF'],\n",
    "                'relative_power_hf': fp_data['relative_power_HF'],\n",
    "                'relative_power_uhf': fp_data['relative_power_UHF'],\n",
    "                'LF_norm': fp_data['HF_norm'],\n",
    "                'HF_norm': fp_data['HF_norm'],\n",
    "            })\n",
    "\n",
    "            dataframes.append(df)\n",
    "            tqdm.update(pbar, step_size)\n",
    "        \n",
    "    result = pd.concat(dataframes, ignore_index=True)\n",
    "    result.to_csv(f'./data/wesad_features_60s/{participant}.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import Parallel, delayed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Parallel(n_jobs=4)(delayed(process_and_save_participant)(participant) for participant in participants) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
