{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "The preprocessing pipelines are defined in this notebook. Throughout the project, two pipelines are mainly used, the pipeline that cleans the data, so that it can be used to extract other features, and the pipeline that extracts the `signal` and `label` for a neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sia import Preprocessing\n",
    "from sia.io import Metadata, read_edf, read_csv, write_csv\n",
    "from sia.preprocessors import neurokit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sia.encoders import GroupEncoder\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning Pipeline\n",
    "The pipeline defined below uses the default preprocessor of Neurokit to clean the data. Furthermore, the data with a low quality is also discarded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Preprocessing() \\\n",
    "    .data(\n",
    "        read_edf(\n",
    "            './data/raw/*.edf', \n",
    "            Metadata('./data/raw/TimeStamps_Merged.txt').on_regex(r'[0-9]{5}')\n",
    "        )\n",
    "    ) \\\n",
    "    .rename({'category': 'label'}) \\\n",
    "    .encode({'label': 'category'}, GroupEncoder({\n",
    "        'baseline': ['Sitting', 'Recov1', 'Recov2', 'Recov3', 'Recov4', 'Recov5', 'Recov6'],\n",
    "        'mental_stress': ['TA', 'SSST_Sing_countdown', 'Pasat', 'Raven', 'TA_repeat', 'Pasat_repeat'],\n",
    "        'high_physical_activity': ['Treadmill1', 'Treadmill2', 'Treadmill3', 'Treadmill4', 'Walking_fast_pace', 'Cycling', 'stairs_up_and_down'],\n",
    "        'moderate_physical_activity': ['Walking_own_pace', 'Dishes', 'Vacuum'],\n",
    "        'low_physical_activity': ['Standing', 'Lying_supine', 'Recov_standing']\n",
    "    })) \\\n",
    "    .filter(lambda category: [_category != None for _category in category]) \\\n",
    "    .process(neurokit()) \\\n",
    "    .filter(lambda ECG_Quality: [quality > .25 for quality in ECG_Quality]) \\\n",
    "    .to(write_csv('./data/cleaned/[0-9]{5}.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### HuggingFace Loader\n",
    "The project often uses HuggingFace Dataset to load the data. To load datadirectories, [HuggingFace Dataset requires a dataloader file](https://huggingface.co/docs/datasets/en/loading) to know how the data is structured.\n",
    " \n",
    "SiA-Kit defines a default \"multi-participant\" dataloader in `sia.config` that can be used to load the data. This can be used by creating a file in the written directory with the same name as the directory and the extension `.py`. \n",
    "\n",
    "An example can be seen below,\n",
    "```python\n",
    "## Filename, `cleaned.py`\n",
    "from datasets.packaged_modules import csv\n",
    "from datasets import Features, Value\n",
    "from sia.config import MultiParticipant, MultiParticipantConfig\n",
    "\n",
    "class CleanedConfig(MultiParticipantConfig, csv.CsvConfig):\n",
    "    def __init__(\n",
    "        self,\n",
    "        train_participants: list[Union[str, int]] = [],\n",
    "        val_participants: list[Union[str, int]] = [],\n",
    "        test_participants: list[Union[str, int]] = [],\n",
    "        *args,\n",
    "        **kwargs\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        train_participants : list[Union[str, int]]\n",
    "            List of participants to be used for training.\n",
    "        val_participants : list[Union[str, int]]\n",
    "            List of participants to be used for validation. \n",
    "        test_participants : list[Union[str, int]]\n",
    "            List of participants to be used for testing.\n",
    "        \"\"\"\n",
    "        MultiParticipantConfig.__init__(self, train_participants, val_participants, test_participants)\n",
    "        csv.CsvConfig.__init__(self, *args, **kwargs)\n",
    "\n",
    "class Cleaned(MultiParticipant, csv.Csv):\n",
    "    BUILDER_CONFIG_CLASS = CleanedConfig\n",
    "\n",
    "    BUILDER_CONFIGS = [\n",
    "        CleanedConfig(\n",
    "            name=\"cleaned\", \n",
    "            description=\"Cleaned data\",\n",
    "            version=\"1.0.0\", \n",
    "            skiprows=1,\n",
    "            column_names=[\n",
    "                'category',\n",
    "                ...\n",
    "            ],\n",
    "            features=Features({\n",
    "                'category': Value('string'),\n",
    "                ...\n",
    "            })\n",
    "        )\n",
    "    ]\n",
    "\n",
    "    DEFAULT_CONFIG_NAME = \"cleaned\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network \n",
    "For the neural network, only the signal and the label is extracted and saved in another directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Preprocessing() \\\n",
    "    .data(read_csv('./data/cleaned/*.csv', columns=['ECG_Clean', 'category'])) \\\n",
    "    .rename({'ECG_Clean': 'signal'}) \\\n",
    "    .encode('category', LabelEncoder()) \\\n",
    "    .to(write_csv('./data/signal/[0-9]{5}.csv'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
